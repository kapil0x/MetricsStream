{
  "name": "MetricStream Performance Optimization",
  "type": "root",
  "description": "Journey from 200 RPS to high-performance metrics platform",
  "children": [
    {
      "name": "Phase 1",
      "type": "phase",
      "description": "Optimization phase: Phase 1",
      "children": [
        {
          "name": "Sequential request processing limiting throughput ...",
          "type": "decision",
          "problem": "Sequential request processing limiting throughput to ~200 RPS with poor multi-client performance",
          "solution": "Thread-per-request model: spawn new thread for each incoming HTTP request",
          "rationale": "Simple implementation providing immediate parallelism gains. Each request gets dedicated thread, avoiding head-of-line blocking. Acceptable for initial scale (<100 concurrent clients)",
          "performance_impact": "20 clients: 81% → 88% success rate. Enables multiple clients to be processed simultaneously",
          "session_date": "2024-09-28",
          "conversation_ref": "Session 4",
          "files_affected": [
            "src/http_server.cpp",
            "src/main.cpp"
          ],
          "children": [
            {
              "name": "Alternative 1",
              "type": "alternative",
              "description": "1) Sequential processing - too slow; 2) Thread pool - more complex",
              "chosen": true,
              "outcome": "Thread-per-request model: spawn new thread for each incoming HTTP request"
            },
            {
              "name": "Alternative 2",
              "type": "alternative",
              "description": "premature optimization; 3) Async I/O - requires complete rewrite",
              "chosen": false,
              "outcome": "Not implemented"
            }
          ]
        }
      ]
    },
    {
      "name": "Phase 2",
      "type": "phase",
      "description": "Optimization phase: Phase 2",
      "children": [
        {
          "name": "File I/O blocking request threads, causing timeout...",
          "type": "decision",
          "problem": "File I/O blocking request threads, causing timeouts under load. Each request thread waiting on disk writes",
          "solution": "Producer-consumer pattern: request threads enqueue batches to concurrent queue, dedicated background thread performs batch writes",
          "rationale": "Decouples critical path (request processing) from I/O operations. Background writer batches multiple metrics for efficient disk writes. Non-blocking enqueue keeps request threads responsive",
          "performance_impact": "50 clients: 59% → 66% success rate. Reduced request latency by eliminating I/O wait time",
          "session_date": "2024-09-29",
          "conversation_ref": "Session 5",
          "files_affected": [
            "src/ingestion_service.cpp",
            "include/ingestion_service.h"
          ],
          "children": [
            {
              "name": "Alternative 1",
              "type": "alternative",
              "description": "1) Synchronous writing - causes request timeouts; 2) Per-request file handles - resource exhaustion; 3) Memory-only buffer - data loss risk",
              "chosen": true,
              "outcome": "Producer-consumer pattern: request threads enqueue batches to concurrent queue, dedicated background thread performs batch writes"
            }
          ]
        }
      ]
    },
    {
      "name": "Phase 3",
      "type": "phase",
      "description": "Optimization phase: Phase 3",
      "children": [
        {
          "name": "JSON parsing becoming bottleneck at high request r...",
          "type": "decision",
          "problem": "JSON parsing becoming bottleneck at high request rates. Regex overhead excessive for simple, known structure",
          "solution": "Custom string-search parser optimized for metric JSON structure: {timestamp, name, value}. Linear scan with field extraction",
          "rationale": "Known metric structure allows optimization. String search is O(n) vs regex O(n²). Eliminates library overhead. Maintains JSON compatibility for ecosystem integration",
          "performance_impact": "100 clients: 80.2% success rate, 2.73ms avg latency. Significant parsing speedup eliminated CPU bottleneck",
          "session_date": "2024-09-30",
          "conversation_ref": "Session 6",
          "files_affected": [
            "src/ingestion_service.cpp"
          ],
          "children": [
            {
              "name": "Alternative 1",
              "type": "alternative",
              "description": "1) Regex-based parsing - O(n²) complexity",
              "chosen": true,
              "outcome": "Custom string-search parser optimized for metric JSON structure: {timestamp, name, value}. Linear scan with field extraction"
            },
            {
              "name": "Alternative 2",
              "type": "alternative",
              "description": "slow; 2) Full JSON library - overkill for simple structure; 3) Binary format - breaks HTTP/JSON compatibility",
              "chosen": false,
              "outcome": "Not implemented"
            }
          ]
        }
      ]
    },
    {
      "name": "Phase 4",
      "type": "phase",
      "description": "Optimization phase: Phase 4",
      "children": [
        {
          "name": "Double mutex serialization: rate_limiter_mutex_ fo...",
          "type": "decision",
          "problem": "Double mutex serialization: rate_limiter_mutex_ followed by metrics_mutex_ serializes all client requests",
          "solution": "Hash-based pre-allocated mutex pool with per-client locking using std::array<std::mutex, MUTEX_POOL_SIZE>",
          "rationale": "Eliminates constructor races while enabling true per-client concurrency",
          "performance_impact": "Expected 50-80% improvement in multi-client scenarios",
          "session_date": "2024-10-01",
          "conversation_ref": "Session 1",
          "files_affected": [
            "include/ingestion_service.h",
            "src/ingestion_service.cpp"
          ],
          "children": [
            {
              "name": "Alternative 1",
              "type": "alternative",
              "description": "Remove mutexes entirely (data corruption risk)",
              "chosen": true,
              "outcome": "Hash-based pre-allocated mutex pool with per-client locking using std::array<std::mutex, MUTEX_POOL_SIZE>"
            },
            {
              "name": "Alternative 2",
              "type": "alternative",
              "description": "std::map<string",
              "chosen": false,
              "outcome": "Not implemented"
            },
            {
              "name": "Alternative 3",
              "type": "alternative",
              "description": "mutex> (constructor races)",
              "chosen": false,
              "outcome": "Not implemented"
            }
          ]
        }
      ]
    },
    {
      "name": "Phase 5",
      "type": "phase",
      "description": "Optimization phase: Phase 5",
      "children": [
        {
          "name": "Profiling revealed thread creation overhead (500μs...",
          "type": "decision",
          "problem": "Profiling revealed thread creation overhead (500μs) dwarfs actual request work (5μs). Thread-per-request model doesn't scale beyond ~2K RPS",
          "solution": "1) Lock-free ring buffer for metrics collection (eliminate mutex contention); 2) Profiling identified need for thread pool architecture (next phase)",
          "rationale": "Lock-free ring buffer eliminates remaining mutex contention in hot path. Profiling data proves thread creation (100x overhead) is the bottleneck. Thread pool will amortize creation cost across requests",
          "performance_impact": "Lock-free collection: reduced contention. Profiling insight: thread pool expected to enable 10K+ RPS by eliminating per-request thread creation",
          "session_date": "2025-10-03",
          "conversation_ref": "Session 7",
          "files_affected": [
            "src/ingestion_service.cpp",
            "include/ingestion_service.h"
          ],
          "children": [
            {
              "name": "Alternative 1",
              "type": "alternative",
              "description": "1) Keep thread-per-request - hits wall at 2K RPS; 2) Event loop (Node.js style) - requires complete rewrite; 3) Thread pool - optimal",
              "chosen": true,
              "outcome": "1) Lock-free ring buffer for metrics collection (eliminate mutex contention); 2) Profiling identified need for thread pool architecture (next phase)"
            },
            {
              "name": "Alternative 2",
              "type": "alternative",
              "description": "planned for Phase 6",
              "chosen": false,
              "outcome": "Not implemented"
            }
          ]
        }
      ]
    },
    {
      "name": "Phase 6",
      "type": "phase",
      "description": "Optimization phase: Phase 6",
      "children": [
        {
          "name": "Thread creation overhead (500μs) dwarfs actual req...",
          "type": "decision",
          "problem": "Thread creation overhead (500μs) dwarfs actual request work (5μs). Thread-per-request doesn't scale beyond 2K RPS",
          "solution": "Thread pool with fixed worker threads: pre-allocate threads, queue incoming requests, workers process from queue",
          "rationale": "Thread pool amortizes creation cost across thousands of requests. Fixed worker count prevents resource exhaustion. Lock-free queue enables non-blocking enqueue from accept thread",
          "performance_impact": "Expected 10K+ RPS by eliminating 96% of thread creation overhead. Actual: revealed listen backlog bottleneck (Phase 7 discovery)",
          "session_date": "2025-10-04",
          "conversation_ref": "Session 10",
          "files_affected": [
            "src/http_server.cpp",
            "include/thread_pool.h",
            "src/thread_pool.cpp"
          ],
          "children": [
            {
              "name": "Alternative 1",
              "type": "alternative",
              "description": "1) Keep thread-per-request - hits wall at 2K RPS; 2) Event loop (Node.js style) - requires complete rewrite; 3) Thread pool - optimal balance",
              "chosen": true,
              "outcome": "Thread pool with fixed worker threads: pre-allocate threads, queue incoming requests, workers process from queue"
            }
          ]
        }
      ]
    },
    {
      "name": "Phase 7",
      "type": "phase",
      "description": "Optimization phase: Phase 7",
      "children": [
        {
          "name": "Success rate stuck at 46-51% despite thread pool o...",
          "type": "decision",
          "problem": "Success rate stuck at 46-51% despite thread pool optimization. 100 concurrent clients overwhelming kernel accept queue (backlog=10). No persistent connection support causing per-request TCP handshake overhead",
          "solution": "Two-part fix: 1) HTTP Keep-Alive loop to reuse connections, 2) Increase listen backlog from 10 to 1024",
          "rationale": "Listen backlog controls kernel queue for pending connections. 100 simultaneous clients need 100 queue slots. HTTP Keep-Alive eliminates per-request handshake (1-2ms overhead). Both system-level and protocol-level optimizations required",
          "performance_impact": "46-51% → 100% success rate (+49-54pp). Latency: 2.05ms → 0.65ms (new conn) / 0.25ms (persistent). Critical lesson: one parameter change (10→1024) fixed everything",
          "session_date": "2025-10-04",
          "conversation_ref": "Session 11",
          "files_affected": [
            "src/http_server.cpp",
            "load_test_persistent.cpp",
            "CMakeLists.txt"
          ],
          "children": [
            {
              "name": "Alternative 1",
              "type": "alternative",
              "description": "1) Keep small backlog - connection refusals continue; 2) Only add Keep-Alive - doesn't fix concurrent connection burst; 3) Both fixes - solved completely",
              "chosen": true,
              "outcome": "Two-part fix: 1) HTTP Keep-Alive loop to reuse connections, 2) Increase listen backlog from 10 to 1024"
            }
          ]
        }
      ]
    }
  ],
  "metadata": {
    "generated_at": "2025-10-04T10:29:22.638353",
    "generator": "MetricStream Decision Tree Generator",
    "version": "1.0"
  }
}