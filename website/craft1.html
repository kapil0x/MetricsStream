<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Craft #1: Metrics Ingestion - Systems Craft</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
            min-height: 100vh;
            line-height: 1.6;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            color: white;
            padding: 60px 40px;
            border-radius: 15px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.2);
            margin-bottom: 30px;
            text-align: center;
        }

        header h1 {
            font-size: 3.5em;
            font-weight: 300;
            margin-bottom: 15px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        header .subtitle {
            font-size: 1.4em;
            opacity: 0.9;
            margin-bottom: 10px;
        }

        header .tagline {
            font-size: 1.1em;
            opacity: 0.7;
            font-style: italic;
        }

        .phase-nav {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            padding: 25px 20px;
            border-radius: 12px;
            box-shadow: 0 8px 25px rgba(0,0,0,0.3);
            margin-bottom: 30px;
        }

        .nav-section {
            margin-bottom: 20px;
        }

        .nav-section:last-child {
            margin-bottom: 0;
        }

        .nav-section-title {
            color: #7f8c8d;
            font-size: 0.75em;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 10px;
            padding-left: 5px;
            font-weight: 600;
        }

        .nav-row {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
        }

        .phase-tab {
            position: relative;
            padding: 12px 18px;
            background: rgba(255, 255, 255, 0.08);
            border: 1.5px solid rgba(255, 255, 255, 0.15);
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.3s ease;
            text-align: center;
            font-size: 0.85em;
            color: #bdc3c7;
            min-width: 100px;
            backdrop-filter: blur(10px);
        }

        .phase-tab:hover {
            transform: translateY(-2px);
            background: rgba(255, 255, 255, 0.12);
            border-color: rgba(255, 255, 255, 0.3);
            box-shadow: 0 4px 12px rgba(52, 152, 219, 0.3);
            color: #ecf0f1;
        }

        .phase-tab.active {
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
            color: white;
            border-color: #3498db;
            box-shadow: 0 4px 15px rgba(52, 152, 219, 0.5);
            font-weight: 600;
        }

        .phase-tab .phase-number {
            font-size: 1.1em;
            font-weight: 700;
            display: block;
            margin-bottom: 4px;
            opacity: 0.9;
        }

        .phase-tab .phase-label {
            font-size: 0.9em;
            opacity: 0.85;
        }

        .phase-tab.active .phase-number,
        .phase-tab.active .phase-label {
            opacity: 1;
        }

        /* Progress indicator */
        .phase-tab.completed::after {
            content: '✓';
            position: absolute;
            top: 4px;
            right: 6px;
            color: #27ae60;
            font-size: 0.9em;
            font-weight: bold;
        }

        .phase-tab.active.completed::after {
            color: #a3f7bf;
        }

        .content-panel {
            display: none;
            background: white;
            border-radius: 15px;
            padding: 50px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            animation: fadeIn 0.5s ease-in;
        }

        .content-panel.active {
            display: block;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        h2 {
            font-size: 2.5em;
            color: #2c3e50;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        h3 {
            font-size: 1.8em;
            color: #34495e;
            margin: 30px 0 15px 0;
        }

        h4 {
            font-size: 1.3em;
            color: #7f8c8d;
            margin: 20px 0 10px 0;
        }

        .info-box {
            background: linear-gradient(135deg, #e8f4f8 0%, #f0f8ff 100%);
            border-left: 4px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .info-box ul {
            color: #2c3e50;
        }

        .info-box ul li {
            color: #2c3e50;
            margin: 8px 0;
        }

        /* Dark info boxes (with inline dark background) */
        .info-box[style*="background: #1a"] ul,
        .info-box[style*="background: #1a"] ul li {
            color: #ecf0f1 !important;
        }

        .warning-box {
            background: linear-gradient(135deg, #fff4e6 0%, #fffaf0 100%);
            border-left: 4px solid #f39c12;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .success-box {
            background: linear-gradient(135deg, #e8f8f5 0%, #f0fff4 100%);
            border-left: 4px solid #27ae60;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .architecture-diagram {
            background: #f8f9fa;
            border: 2px solid #dee2e6;
            border-radius: 10px;
            padding: 30px;
            margin: 30px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }

        .architecture-diagram pre {
            font-size: 0.9em;
            line-height: 1.8;
            color: #2c3e50;
        }

        .code-block {
            background: #282c34;
            color: #abb2bf;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Monaco', 'Menlo', 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.6;
        }

        .code-block pre {
            margin: 0;
            white-space: pre;
            font-family: inherit;
            color: inherit;
        }

        .code-block .comment {
            color: #5c6370;
            font-style: italic;
        }

        .code-block .keyword {
            color: #c678dd;
            font-weight: bold;
        }

        .code-block .string {
            color: #98c379;
        }

        .code-block .function {
            color: #61afef;
        }

        .code-block .number {
            color: #d19a66;
        }

        .lab-section {
            background: linear-gradient(135deg, #fef5e7 0%, #fff9f0 100%);
            border: 2px solid #f39c12;
            border-radius: 10px;
            padding: 30px;
            margin: 30px 0;
        }

        .lab-section h4 {
            color: #d68910;
            margin-top: 0;
        }

        .command {
            background: #2c3e50;
            color: #2ecc71;
            padding: 15px;
            border-radius: 6px;
            font-family: monospace;
            margin: 10px 0;
            border-left: 4px solid #27ae60;
        }

        .metrics-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .metrics-table th {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        .metrics-table td {
            padding: 12px 15px;
            border-bottom: 1px solid #e9ecef;
        }

        .metrics-table tr:nth-child(even) {
            background: #f8f9fa;
        }

        .metrics-table tr:hover {
            background: #e8f4f8;
        }

        .improvement {
            color: #27ae60;
            font-weight: bold;
        }

        .regression {
            color: #e74c3c;
            font-weight: bold;
        }

        ul, ol {
            margin: 15px 0 15px 30px;
        }

        li {
            margin: 8px 0;
        }

        .badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 600;
            margin: 0 5px;
        }

        .badge-success {
            background: #27ae60;
            color: white;
        }

        .badge-warning {
            background: #f39c12;
            color: white;
        }

        .badge-info {
            background: #3498db;
            color: white;
        }

        .badge-danger {
            background: #e74c3c;
            color: white;
        }

        .decision-point {
            background: linear-gradient(135deg, #fef5e7 0%, #fff4e0 100%);
            border: 2px dashed #f39c12;
            border-radius: 10px;
            padding: 25px;
            margin: 30px 0;
        }

        .decision-point h4 {
            color: #d68910;
            margin-top: 0;
        }

        .alternatives {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }

        .alternative {
            background: white;
            border: 2px solid #e9ecef;
            border-radius: 8px;
            padding: 15px;
            transition: all 0.3s ease;
        }

        .alternative:hover {
            border-color: #667eea;
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.2);
        }

        .alternative.chosen {
            border-color: #27ae60;
            background: linear-gradient(135deg, #e8f8f5 0%, #f0fff4 100%);
        }

        .alternative h5 {
            margin: 0 0 10px 0;
            color: #2c3e50;
        }

        .alternative.chosen h5 {
            color: #27ae60;
        }

        .architecture-comparison {
            display: grid;
            grid-template-columns: 1fr auto 1fr;
            gap: 20px;
            margin: 30px 0;
            align-items: center;
        }

        .arch-before, .arch-after {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
        }

        .arch-before {
            border: 2px solid #e74c3c;
        }

        .arch-after {
            border: 2px solid #27ae60;
        }

        .arch-arrow {
            font-size: 3em;
            color: #667eea;
            text-align: center;
        }

        .footer {
            background: linear-gradient(135deg, #2c3e50 0%, #34495e 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin-top: 40px;
            text-align: center;
        }

        .phase-progress {
            background: white;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .progress-bar {
            background: #e9ecef;
            height: 30px;
            border-radius: 15px;
            overflow: hidden;
            margin: 10px 0;
        }

        .progress-fill {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            height: 100%;
            transition: width 0.5s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
        }

        .solution-section {
            background: #f8f9fa;
            border: 2px dashed #95a5a6;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }

        .solution-hidden {
            display: none;
        }

        .solution-visible {
            display: block;
            animation: fadeIn 0.5s ease-in;
        }

        .show-solution-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 12px 24px;
            border-radius: 8px;
            cursor: pointer;
            font-weight: 600;
            font-size: 1em;
            transition: all 0.3s ease;
            margin: 10px 0;
        }

        .show-solution-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }

        .show-solution-btn:active {
            transform: translateY(0);
        }

        .hint-section {
            background: linear-gradient(135deg, #fff9e6 0%, #fffbf0 100%);
            border-left: 4px solid #f39c12;
            padding: 15px;
            margin: 15px 0;
            border-radius: 8px;
            font-size: 0.95em;
        }

        .hint-section h5 {
            color: #d68910;
            margin-top: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <a href="systems-craft-main.html" style="display: inline-block; color: #95a5a6; text-decoration: none; margin-bottom: 15px; font-size: 0.9em; transition: color 0.3s ease;" onmouseover="this.style.color='#ecf0f1'" onmouseout="this.style.color='#95a5a6'">
                ← Back to Systems Craft
            </a>
            <div style="font-size: 0.9em; color: #95a5a6; margin-bottom: 10px; letter-spacing: 2px;">SYSTEMS CRAFT</div>
            <h1>⚡ Craft #1: Metrics Ingestion</h1>
            <p class="subtitle">Build a high-performance metrics system from scratch</p>
            <p class="tagline">A hands-on tutorial teaching concurrent programming and performance optimization through building a complete metrics platform</p>
        </header>

        <div class="phase-nav">
            <!-- Getting Started Section -->
            <div class="nav-section">
                <div class="nav-section-title">📖 Getting Started</div>
                <div class="nav-row">
                    <div class="phase-tab active" onclick="showPhase('overview')">
                        <span class="phase-number">📚</span>
                        <span class="phase-label">Overview</span>
                    </div>
                    <div class="phase-tab" onclick="showPhase('phase0')">
                        <span class="phase-number">0</span>
                        <span class="phase-label">System Design</span>
                    </div>
                </div>
            </div>

            <!-- Milestone 1: Phases 1-7 -->
            <div class="nav-section">
                <div class="nav-section-title">🚀 Milestone 1: High-Performance Ingestion (2,253 RPS)</div>
                <div class="nav-row">
                    <div class="phase-tab completed" onclick="showPhase('phase1')">
                        <span class="phase-number">1</span>
                        <span class="phase-label">Threading</span>
                    </div>
                    <div class="phase-tab completed" onclick="showPhase('phase2')">
                        <span class="phase-number">2</span>
                        <span class="phase-label">Async I/O</span>
                    </div>
                    <div class="phase-tab completed" onclick="showPhase('phase3')">
                        <span class="phase-number">3</span>
                        <span class="phase-label">JSON Parser</span>
                    </div>
                    <div class="phase-tab completed" onclick="showPhase('phase4')">
                        <span class="phase-number">4</span>
                        <span class="phase-label">Mutex Pool</span>
                    </div>
                    <div class="phase-tab completed" onclick="showPhase('phase5')">
                        <span class="phase-number">5</span>
                        <span class="phase-label">Thread Pool</span>
                    </div>
                    <div class="phase-tab completed" onclick="showPhase('phase6')">
                        <span class="phase-number">6</span>
                        <span class="phase-label">Ring Buffer</span>
                    </div>
                    <div class="phase-tab completed" onclick="showPhase('phase7')">
                        <span class="phase-number">7</span>
                        <span class="phase-label">Keep-Alive</span>
                    </div>
                    <div class="phase-tab completed" onclick="showPhase('phase8')">
                        <span class="phase-number">8</span>
                        <span class="phase-label">Event-Driven I/O</span>
                    </div>
                </div>
            </div>

            <!-- What's Next Section -->
            <div class="nav-section">
                <div class="nav-section-title">🔮 What's Next</div>
                <div class="nav-row">
                    <div class="phase-tab" onclick="showPhase('future')">
                        <span class="phase-number">🔮</span>
                        <span class="phase-label">Future Milestones</span>
                    </div>
                    <div class="phase-tab" onclick="showPhase('decision-tree')">
                        <span class="phase-number">🌳</span>
                        <span class="phase-label">Decision Tree</span>
                    </div>
                </div>
            </div>
        </div>

        <!-- Overview Panel -->
        <div class="content-panel active" id="overview">
            <h2>🎯 Learning Journey Overview</h2>

            <div class="lab-section" style="background: linear-gradient(135deg, #27ae60 0%, #229954 100%); border: none; margin-bottom: 30px;">
                <h3 style="color: #fff; margin-bottom: 15px;">🚀 Quick Start (2 minutes)</h3>

                <div style="background: rgba(255,255,255,0.95); border-radius: 8px; padding: 20px; color: #2c3e50;">
                    <h4 style="color: #27ae60; margin-bottom: 15px;">Step 1: Fork & Clone</h4>
                    <div class="command" style="background: #2c3e50; color: #ecf0f1; margin-bottom: 10px;">
# Fork the repository on GitHub first:
# https://github.com/kapil0x/systems-craft

# Then clone YOUR fork
$ git clone https://github.com/YOUR_USERNAME/systems-craft.git
$ cd systems-craft</div>

                    <h4 style="color: #27ae60; margin-top: 20px; margin-bottom: 15px;">Step 2: Build & Run</h4>
                    <div class="command" style="background: #2c3e50; color: #ecf0f1; margin-bottom: 10px;">
# Build the complete Phase 7 version
$ mkdir -p build
$ cmake -B build
$ make -C build metricstream_server load_test

# Run the server
$ ./build/metricstream_server

# In another terminal, run a load test
$ ./build/load_test 8080 20 10</div>

                    <h4 style="color: #27ae60; margin-top: 20px; margin-bottom: 15px;">Step 3: Start Learning</h4>
                    <p style="color: #34495e; margin-bottom: 10px;">Choose your path:</p>
                    <ul style="color: #34495e; margin-left: 20px; line-height: 1.8;">
                        <li><strong>Guided Learning:</strong> Click "Phase 1" above to start the tutorial</li>
                        <li><strong>Hands-On Practice:</strong> Checkout <code style="background: #ecf0f1; padding: 2px 6px; border-radius: 3px;">git checkout phase-1-starter</code> and implement yourself</li>
                        <li><strong>Code Review:</strong> Read the solution branches to understand each optimization</li>
                    </ul>
                </div>

                <p style="color: #fff; margin-top: 15px; font-size: 0.9em; opacity: 0.9;">
                    💡 <strong>Pro Tip:</strong> Fork the repo so you can commit your own implementations as you work through each phase!
                </p>
            </div>

            <div class="info-box">
                <h3>What is this?</h3>
                <p>A hands-on tutorial for building a production-grade metrics ingestion system from scratch. You'll write the code, run performance tests, and see real bottlenecks emerge—using the same engineering methodology as infrastructure teams at Google, Meta, Amazon, and Netflix.</p>

                <div style="margin-top: 15px; padding: 15px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 8px;">
                    <p style="color: #fff; margin: 0; line-height: 1.7;">
                        <strong style="color: #f1c40f;">💼 Production Engineering Approach:</strong> This teaches the same systematic methodology used to build production systems at Big Tech companies. Engineers don't use frameworks—they build custom solutions optimized for their specific scale. You'll learn: measure bottlenecks with profiling → design targeted solutions → implement optimizations → validate with load tests. This is production engineering.
                    </p>
                </div>
            </div>

            <div class="warning-box" style="margin-top: 20px;">
                <h4>⚠️ What This Is NOT</h4>
                <ul>
                    <li>❌ <strong>Complete monitoring platform</strong> - Only the ingestion component (Craft #1 of series)</li>
                    <li>❌ <strong>Prometheus/DataDog replacement</strong> - Missing storage, query engine, visualization</li>
                    <li>❌ <strong>Production deployment ready</strong> - No TLS, authentication, monitoring, or graceful shutdown</li>
                    <li>❌ <strong>Absolute performance benchmark</strong> - Measured on localhost; real-world has network latency</li>
                </ul>

                <h4 style="margin-top: 15px;">✅ What This IS</h4>
                <ul>
                    <li>✅ <strong>Production-grade engineering</strong> - Real concurrency patterns, lock-free data structures, performance optimization</li>
                    <li>✅ <strong>Systematic optimization methodology</strong> - Profile → measure → optimize → validate (Big Tech approach)</li>
                    <li>✅ <strong>Foundation for real systems</strong> - Core patterns used in production infrastructure at scale</li>
                    <li>✅ <strong>Hands-on implementation</strong> - You write the code, see the bottlenecks, implement the fixes</li>
                </ul>
            </div>

            <div class="info-box" style="margin-top: 20px; background: #f0f8ff;">
                <h4>📊 Test Environment</h4>
                <p><strong>Hardware:</strong> MacBook Pro M1 (8-core, 16GB RAM)<br>
                <strong>OS:</strong> macOS 14.2<br>
                <strong>Network:</strong> localhost loopback (no network latency)<br>
                <strong>Load:</strong> Synthetic traffic from load_test tool<br>
                <strong>Duration:</strong> 60-second sustained tests</p>

                <p style="margin-top: 10px;"><strong>Note:</strong> Production deployments on AWS c5.2xlarge showed 15-20% higher throughput due to more cores and better I/O. Your results will vary based on hardware.</p>
            </div>

            <h3>🏗️ Craft #1: Complete Platform Architecture</h3>

            <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 20px 0;">
                <p style="color: rgba(255,255,255,0.95); text-align: center; font-size: 1.1em; line-height: 1.7; margin-bottom: 25px;">
                    Craft #1 builds a <strong>complete metrics platform</strong> through 4 progressive milestones. Milestone 1 (Phases 1-7) is complete and ready to use. The remaining milestones show the path to a production-ready monitoring system.
                </p>

                <!-- Milestone 1 - COMPLETE -->
                <div style="background: linear-gradient(135deg, #27ae60 0%, #229954 100%); border-radius: 12px; padding: 20px; margin-bottom: 15px; box-shadow: 0 6px 20px rgba(39,174,96,0.4); border: 3px solid #f1c40f;">
                    <div style="display: flex; align-items: center; margin-bottom: 12px;">
                        <div style="background: #f1c40f; color: #27ae60; width: 45px; height: 45px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.3em; margin-right: 15px; box-shadow: 0 3px 10px rgba(241,196,15,0.5);">1</div>
                        <div>
                            <div style="color: #f1c40f; font-size: 0.75em; font-weight: 600; letter-spacing: 1px;">✓ MILESTONE COMPLETE</div>
                            <div style="color: #fff; font-size: 1.3em; font-weight: bold;">High-Performance Ingestion Layer</div>
                        </div>
                        <span style="margin-left: auto; background: #f1c40f; padding: 6px 14px; border-radius: 20px; color: #27ae60; font-weight: bold; font-size: 0.9em;">2,253 RPS</span>
                    </div>
                    <div style="color: rgba(255,255,255,0.95); font-size: 0.95em; line-height: 1.6; padding-left: 60px;">
                        <strong>What you built:</strong> HTTP server from raw sockets → Thread pool → Lock-free ring buffers → Custom JSON parser → Rate limiting<br>
                        <strong>Performance:</strong> 2,253 RPS sustained, 100% reliability, p50 = 0.25ms latency<br>
                        <strong>Phases completed:</strong> 7 progressive optimizations (Phases 1-7)<br>
                        <strong>Skills mastered:</strong> Concurrent programming, performance optimization, systems profiling
                    </div>
                </div>

                <div style="margin-top: 30px; padding: 25px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 12px; color: white;">
                    <h4 style="color: white; margin-top: 0; margin-bottom: 15px;">🗺️ Your Learning Journey: 6 Crafts</h4>
                    <p style="margin-bottom: 20px; opacity: 0.95; line-height: 1.7;">
                        Craft #1 teaches you metrics ingestion with 8 optimization phases. Then continue building the complete monitoring platform:
                    </p>
                    <div style="display: grid; gap: 12px;">
                        <div style="background: rgba(255,255,255,0.15); padding: 15px; border-radius: 8px; border-left: 4px solid #f1c40f;">
                            <strong>Craft #0: Monitoring Platform PoC</strong> (Coming Soon)<br>
                            <span style="opacity: 0.9; font-size: 0.9em;">End-to-end system in an afternoon - see the big picture</span>
                        </div>
                        <div style="background: rgba(39,174,96,0.3); padding: 15px; border-radius: 8px; border-left: 4px solid #27ae60;">
                            <strong>Craft #1: Metrics Ingestion</strong> (You are here! ✅)<br>
                            <span style="opacity: 0.9; font-size: 0.9em;">HTTP server, rate limiting, async I/O - 200 → 145K RPS</span>
                        </div>
                        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 8px; border-left: 4px solid rgba(255,255,255,0.5);">
                            <strong>Craft #2: Distributed Message Queue</strong> (Coming Soon)<br>
                            <span style="opacity: 0.8; font-size: 0.9em;">Kafka-like system - partitions, replication, consumer groups</span>
                        </div>
                        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 8px; border-left: 4px solid rgba(255,255,255,0.5);">
                            <strong>Craft #3: Time-Series Storage Engine</strong> (Coming Soon)<br>
                            <span style="opacity: 0.8; font-size: 0.9em;">InfluxDB-like database - LSM trees, compression, indexing</span>
                        </div>
                        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 8px; border-left: 4px solid rgba(255,255,255,0.5);">
                            <strong>Craft #4: Query & Aggregation Engine</strong> (Coming Soon)<br>
                            <span style="opacity: 0.8; font-size: 0.9em;">PromQL-like queries - parallel execution, aggregations</span>
                        </div>
                        <div style="background: rgba(255,255,255,0.1); padding: 15px; border-radius: 8px; border-left: 4px solid rgba(255,255,255,0.5);">
                            <strong>Craft #5: Alerting & Notification System</strong> (Coming Soon)<br>
                            <span style="opacity: 0.8; font-size: 0.9em;">PagerDuty-like alerting - rule engine, state machine, webhooks</span>
                        </div>
                    </div>
                    <p style="margin-top: 20px; opacity: 0.95; line-height: 1.7;">
                        <strong>By the end:</strong> You'll have built a complete distributed monitoring platform and understand how each component connects - ingestion → queue → storage → query → alerting.
                    </p>
                </div>
            </div>

            <div class="success-box">
                <h4>✅ What You'll Build in Craft #1</h4>
                <p><strong>8 Progressive Optimization Phases</strong></p>
                <ul style="column-count: 2; column-gap: 30px;">
                    <li><strong>Phase 1:</strong> Thread-per-request model</li>
                    <li><strong>Phase 2:</strong> Async I/O with producer-consumer</li>
                    <li><strong>Phase 3:</strong> Optimized JSON parser (O(n))</li>
                    <li><strong>Phase 4:</strong> Hash-based mutex pool</li>
                    <li><strong>Phase 5:</strong> Thread pool architecture</li>
                    <li><strong>Phase 6:</strong> Lock-free ring buffers</li>
                    <li><strong>Phase 7:</strong> HTTP Keep-Alive + backlog tuning</li>
                    <li><strong>Phase 8:</strong> Event-driven I/O with epoll (64x improvement!)</li>
                </ul>

                <p style="margin-top: 20px;"><strong>Core Components Built:</strong></p>
                <ul style="column-count: 2; column-gap: 30px;">
                    <li>HTTP server (raw Berkeley sockets)</li>
                    <li>Thread pool with task queue</li>
                    <li>Custom JSON parser (zero dependencies)</li>
                    <li>Lock-free atomic ring buffers</li>
                    <li>Sliding window rate limiter</li>
                    <li>HTTP Keep-Alive persistent connections</li>
                    <li>Event loop with epoll (145K RPS!)</li>
                    <li>Non-blocking I/O and edge-triggered events</li>
                </ul>

                <div style="margin-top: 20px; padding: 15px; background: linear-gradient(135deg, #27ae60 0%, #229954 100%); color: white; border-radius: 8px; border-left: 4px solid #fff;">
                    <strong style="color: #fff;">🎯 Phase 7 Performance:</strong> 2,253 RPS | 100% success | p50 = 0.25ms, p99 = 0.65ms<br>
                    <strong style="color: #fff; margin-top: 10px; display: inline-block;">⚡ Phase 8 Performance:</strong> 145,348 RPS | 100% success @ 1000 clients | 4.81ms latency | <strong>64x faster!</strong>
                </div>
            </div>

            <h3>🎓 How to Use This Tutorial</h3>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
                <div style="background: #ecf0f1; padding: 20px; border-radius: 10px; border-left: 4px solid #3498db;">
                    <h4 style="color: #3498db; margin-top: 0;">📖 Guided Learning Path</h4>
                    <ol style="line-height: 1.8; color: #2c3e50;">
                        <li>Read each phase's tutorial above</li>
                        <li>Understand the problem & solution</li>
                        <li>Study the code examples</li>
                        <li>See the performance impact</li>
                    </ol>
                    <p style="margin: 10px 0 0 0; color: #7f8c8d; font-size: 0.9em;"><strong>Best for:</strong> Understanding concepts first</p>
                </div>

                <div style="background: #ecf0f1; padding: 20px; border-radius: 10px; border-left: 4px solid #27ae60;">
                    <h4 style="color: #27ae60; margin-top: 0;">💻 Hands-On Practice</h4>
                    <ol style="line-height: 1.8; color: #2c3e50;">
                        <li><code style="background: #bdc3c7; padding: 2px 6px; border-radius: 3px;">git checkout phase-N-starter</code></li>
                        <li>Read TODO comments in code</li>
                        <li>Implement the optimization yourself</li>
                        <li>Compare with <code style="background: #bdc3c7; padding: 2px 6px; border-radius: 3px;">phase-N-solution</code></li>
                    </ol>
                    <p style="margin: 10px 0 0 0; color: #7f8c8d; font-size: 0.9em;"><strong>Best for:</strong> Learning by doing</p>
                </div>
            </div>

            <div class="info-box" style="background: #fff3cd; border-left: 4px solid #f39c12;">
                <h4 style="color: #f39c12; margin-top: 0;">⚡ Pro Tips</h4>
                <ul style="line-height: 1.8;">
                    <li><strong>Measure everything:</strong> Run load tests before and after each optimization</li>
                    <li><strong>Read the profiling data:</strong> See which optimizations had the biggest impact</li>
                    <li><strong>Work sequentially:</strong> Each phase builds on the previous one</li>
                    <li><strong>Fork the repo:</strong> Commit your implementations as you go</li>
                    <li><strong>Compare approaches:</strong> Check <code>/docs</code> for decision trees explaining why certain designs were chosen</li>
                </ul>
            </div>
        </div>

        <!-- Phase 0: System Design -->
        <div class="content-panel" id="phase0">
            <h2>Phase 0: System Design Foundations</h2>

            <div class="info-box">
                <h3>🎯 What You'll Build</h3>
                <p>Design a complete real-time monitoring system before writing any code. This phase teaches you to think like a system architect: define requirements, design data flow, choose technologies, and calculate capacity.</p>
            </div>

            <h3>Step 1: Define Requirements & Constraints</h3>

            <div class="decision-point">
                <h4>The Problem We're Solving</h4>
                <p>You're running 10,000 servers in production. Each server needs monitoring—CPU usage, memory consumption, disk I/O, network traffic. Without a monitoring system, you're flying blind when things go wrong.</p>

                <h4>What the System Must Do (Functional)</h4>
                <ul>
                    <li><strong>Collect metrics from 10,000 servers</strong> - Every server sends data every 10 seconds</li>
                    <li><strong>Calculate: 10K servers × 10 metrics × 6 collections/min</strong> = 600K data points per minute sustained</li>
                    <li><strong>Store everything for 30 days</strong> - Engineers need to look back when debugging</li>
                    <li><strong>Show live dashboards</strong> - See what's happening right now (<1 second lag)</li>
                    <li><strong>Send alerts</strong> - Page someone when CPU hits 80% for 5 minutes straight</li>
                    <li><strong>Support historical queries</strong> - "What was CPU doing last Tuesday at 3pm?"</li>
                </ul>

                <h4>How Well It Must Perform (Non-Functional)</h4>
                <ul>
                    <li><strong>Throughput:</strong> Handle 100K metrics/sec sustained (10× current load for growth)</li>
                    <li><strong>Availability:</strong> 99.9% uptime = max 43 minutes downtime per month</li>
                    <li><strong>Latency:</strong> Dashboard shows data within 1 second of collection</li>
                    <li><strong>Scalability:</strong> Add servers by scaling horizontally (no redesign needed)</li>
                </ul>

                <h4>Real-World Constraints</h4>
                <ul>
                    <li><strong>Network reality:</strong> Agents run behind firewalls → must use push model (agents initiate connections)</li>
                    <li><strong>Storage economics:</strong> 100K metrics/sec × 30 days = 259 billion data points → need compression</li>
                    <li><strong>Team size:</strong> Small engineering team (2-3 people) → system must be operationally simple</li>
                    <li><strong>Cost:</strong> Self-hosted should be significantly cheaper than SaaS ($10K+/month for this scale)</li>
                </ul>
            </div>

            <h3>Step 2: High-Level System Architecture</h3>

            <div style="background: #2a2a2a; border-radius: 12px; padding: 30px; margin: 20px 0;">

                <!-- Layer 1: Monitoring Agents -->
                <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 10px; padding: 20px; margin-bottom: 15px;">
                    <div style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="background: #fff; color: #667eea; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.2em; margin-right: 15px;">1</div>
                        <h4 style="color: #fff; margin: 0; font-size: 1.2em;">MONITORING AGENTS</h4>
                        <span style="margin-left: auto; background: rgba(255,255,255,0.2); padding: 5px 12px; border-radius: 20px; color: #fff; font-size: 0.9em;">10,000 servers</span>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 10px; margin-bottom: 15px;">
                        <div style="background: rgba(255,255,255,0.15); padding: 10px; border-radius: 5px; text-align: center; color: #fff; font-size: 0.85em;">Server 1</div>
                        <div style="background: rgba(255,255,255,0.15); padding: 10px; border-radius: 5px; text-align: center; color: #fff; font-size: 0.85em;">Server 2</div>
                        <div style="background: rgba(255,255,255,0.15); padding: 10px; border-radius: 5px; text-align: center; color: #fff; font-size: 0.85em;">Server 3</div>
                        <div style="background: rgba(255,255,255,0.15); padding: 10px; border-radius: 5px; text-align: center; color: #fff; font-size: 0.85em;">... 10K</div>
                    </div>
                    <div style="color: rgba(255,255,255,0.9); font-size: 0.9em; line-height: 1.6;">
                        <strong>Collect every 10 seconds:</strong><br>
                        • CPU usage, memory, disk I/O<br>
                        • Network traffic, process count<br>
                        • Custom application metrics
                    </div>
                </div>

                <div style="text-align: center; color: #3498db; font-size: 1.8em; margin: 15px 0;">↓</div>
                <div style="text-align: center; color: #95a5a6; font-size: 0.95em; margin-bottom: 15px;">
                    <strong>HTTP POST (JSON)</strong><br>
                    10 metrics/server × 10K servers = <span style="color: #3498db; font-weight: bold;">100K metrics/sec</span>
                </div>
                <div style="text-align: center; color: #3498db; font-size: 1.8em; margin: 15px 0;">↓</div>

                <!-- Layer 2: Ingestion Service (IMPLEMENTED) -->
                <div style="background: linear-gradient(135deg, #27ae60 0%, #229954 100%); border-radius: 10px; padding: 20px; margin-bottom: 15px; box-shadow: 0 0 20px rgba(39,174,96,0.3);">
                    <div style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="background: #fff; color: #27ae60; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.2em; margin-right: 15px;">2</div>
                        <h4 style="color: #fff; margin: 0; font-size: 1.2em;">INGESTION SERVICE</h4>
                        <span style="margin-left: auto; background: #fff; padding: 5px 12px; border-radius: 20px; color: #27ae60; font-weight: bold; font-size: 0.9em;">✅ Phase 1-7 Complete</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin-bottom: 12px; color: #fff; text-align: center;">
                        <strong>Load Balancer</strong> (Round-robin)
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 15px;">
                        <div style="background: rgba(255,255,255,0.15); padding: 12px; border-radius: 5px; color: #fff; font-size: 0.85em;">
                            <div style="font-weight: bold; margin-bottom: 5px;">Server 1</div>
                            <div style="color: #f1c40f;">2,253 RPS</div>
                        </div>
                        <div style="background: rgba(255,255,255,0.15); padding: 12px; border-radius: 5px; color: #fff; font-size: 0.85em;">
                            <div style="font-weight: bold; margin-bottom: 5px;">Server 2</div>
                            <div style="color: #f1c40f;">2,253 RPS</div>
                        </div>
                        <div style="background: rgba(255,255,255,0.15); padding: 12px; border-radius: 5px; color: #fff; font-size: 0.85em;">
                            <div style="font-weight: bold; margin-bottom: 5px;">... Server 20</div>
                            <div style="color: #f1c40f;">2,253 RPS</div>
                        </div>
                    </div>
                    <div style="color: rgba(255,255,255,0.9); font-size: 0.9em; line-height: 1.6;">
                        <strong>Features:</strong> HTTP server, rate limiting, JSON parsing, thread pool, async I/O<br>
                        <strong>Performance:</strong> <span style="color: #f1c40f; font-weight: bold;">2,253 RPS per server @ 100% success</span>
                    </div>
                </div>

                <div style="text-align: center; color: #3498db; font-size: 1.8em; margin: 15px 0;">↓</div>
                <div style="text-align: center; color: #95a5a6; font-size: 0.95em; margin-bottom: 15px;">Forward to message queue</div>
                <div style="text-align: center; color: #3498db; font-size: 1.8em; margin: 15px 0;">↓</div>

                <!-- Layer 3: Message Queue -->
                <div style="background: linear-gradient(135deg, #e67e22 0%, #d35400 100%); border-radius: 10px; padding: 20px; margin-bottom: 15px;">
                    <div style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="background: #fff; color: #e67e22; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.2em; margin-right: 15px;">3</div>
                        <h4 style="color: #fff; margin: 0; font-size: 1.2em;">MESSAGE QUEUE</h4>
                        <span style="margin-left: auto; background: rgba(255,255,255,0.2); padding: 5px 12px; border-radius: 20px; color: #fff; font-size: 0.9em;">Phase 8: Planned</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin-bottom: 12px; color: #fff;">
                        <strong style="font-size: 1.05em;">Apache Kafka</strong><br>
                        <span style="font-size: 0.9em; color: rgba(255,255,255,0.9);">3 brokers, 12 partitions</span>
                    </div>
                    <div style="color: rgba(255,255,255,0.9); font-size: 0.9em; line-height: 1.6;">
                        • Partitioning: Hash(client_id) for load distribution<br>
                        • Retention: 7 days for replay capability<br>
                        • Decouples ingestion from processing<br>
                        • Acts as buffer during processing failures
                    </div>
                </div>

                <div style="text-align: center; color: #3498db; font-size: 1.8em; margin: 15px 0;">↓</div>

                <!-- Layer 4: Stream Processing -->
                <div style="background: linear-gradient(135deg, #3498db 0%, #2980b9 100%); border-radius: 10px; padding: 20px; margin-bottom: 15px;">
                    <div style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="background: #fff; color: #3498db; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.2em; margin-right: 15px;">4</div>
                        <h4 style="color: #fff; margin: 0; font-size: 1.2em;">STREAM PROCESSING</h4>
                        <span style="margin-left: auto; background: rgba(255,255,255,0.2); padding: 5px 12px; border-radius: 20px; color: #fff; font-size: 0.9em;">Phase 9: Planned</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin-bottom: 12px; color: #fff;">
                        <strong style="font-size: 1.05em;">Apache Flink</strong><br>
                        <span style="font-size: 0.9em; color: rgba(255,255,255,0.9);">12 workers</span>
                    </div>
                    <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; margin-bottom: 12px;">
                        <div style="background: rgba(255,255,255,0.15); padding: 10px; border-radius: 5px; text-align: center; color: #fff; font-size: 0.85em;">
                            <strong>1-min windows</strong><br>avg, p95, p99
                        </div>
                        <div style="background: rgba(255,255,255,0.15); padding: 10px; border-radius: 5px; text-align: center; color: #fff; font-size: 0.85em;">
                            <strong>Anomaly detect</strong><br>CPU spike >80%
                        </div>
                        <div style="background: rgba(255,255,255,0.15); padding: 10px; border-radius: 5px; text-align: center; color: #fff; font-size: 0.85em;">
                            <strong>Aggregations</strong><br>by host, type
                        </div>
                    </div>
                    <div style="color: rgba(255,255,255,0.9); font-size: 0.9em; line-height: 1.6;">
                        <strong>Outputs:</strong><br>
                        • Raw metrics → Storage (all data points)<br>
                        • Aggregated metrics → Storage (1-min summaries)<br>
                        • Alerts → Notification service (threshold violations)
                    </div>
                </div>

                <div style="text-align: center; color: #3498db; font-size: 1.8em; margin: 15px 0;">↓</div>

                <!-- Layer 5: Storage -->
                <div style="background: linear-gradient(135deg, #9b59b6 0%, #8e44ad 100%); border-radius: 10px; padding: 20px; margin-bottom: 15px;">
                    <div style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="background: #fff; color: #9b59b6; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.2em; margin-right: 15px;">5</div>
                        <h4 style="color: #fff; margin: 0; font-size: 1.2em;">TIME-SERIES STORAGE</h4>
                        <span style="margin-left: auto; background: rgba(255,255,255,0.2); padding: 5px 12px; border-radius: 20px; color: #fff; font-size: 0.9em;">Phase 10: Planned</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin-bottom: 12px; color: #fff;">
                        <strong style="font-size: 1.05em;">InfluxDB Cluster</strong><br>
                        <span style="font-size: 0.9em; color: rgba(255,255,255,0.9);">3 nodes</span>
                    </div>
                    <div style="color: rgba(255,255,255,0.9); font-size: 0.9em; line-height: 1.6;">
                        <strong>Retention policies:</strong><br>
                        • Raw data: 30 days (100K writes/sec)<br>
                        • 1-min aggregates: 1 year (1.6K writes/sec)<br>
                        • 1-hour aggregates: 5 years (27 writes/sec)<br><br>
                        <strong>Why InfluxDB?</strong> Time-series optimized, 10:1 compression, built-in downsampling
                    </div>
                </div>

                <div style="text-align: center; color: #3498db; font-size: 1.8em; margin: 15px 0;">↓</div>

                <!-- Layer 6: Visualization -->
                <div style="background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%); border-radius: 10px; padding: 20px;">
                    <div style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="background: #fff; color: #e74c3c; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; font-size: 1.2em; margin-right: 15px;">6</div>
                        <h4 style="color: #fff; margin: 0; font-size: 1.2em;">VISUALIZATION & ALERTING</h4>
                        <span style="margin-left: auto; background: rgba(255,255,255,0.2); padding: 5px 12px; border-radius: 20px; color: #fff; font-size: 0.9em;">Phase 11: Planned</span>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin-bottom: 12px; color: #fff;">
                        <strong style="font-size: 1.05em;">Grafana Dashboards + Alert Manager</strong>
                    </div>
                    <div style="color: rgba(255,255,255,0.9); font-size: 0.9em; line-height: 1.6;">
                        • Real-time charts (last 1 hour, 1 day, 1 week)<br>
                        • Custom dashboards per team/service<br>
                        • Alert rules with PagerDuty/Slack integration<br>
                        • Threshold violations (CPU > 80% for 5 min)<br>
                        • Anomaly detection & alert aggregation
                    </div>
                </div>

                <!-- Summary -->
                <div style="margin-top: 30px; background: #1e1e1e; border-left: 4px solid #3498db; padding: 20px; border-radius: 5px;">
                    <div style="color: #3498db; font-weight: bold; font-size: 1.1em; margin-bottom: 10px;">📊 Data Flow Summary</div>
                    <div style="color: #95a5a6; font-size: 1em; font-family: monospace; line-height: 1.8;">
                        Agents → Ingestion → Kafka → Flink → InfluxDB → Grafana<br>
                        <span style="color: #27ae60; font-weight: bold;">         ↑ We are here (Phase 1-7 complete)</span>
                    </div>
                </div>
            </div>

            <div class="info-box" style="margin-top: 20px;">
                <h4>🎓 Design Principles</h4>
                <ul>
                    <li><strong>Decouple components:</strong> Message queue separates ingestion from processing—if Flink crashes, Kafka buffers data</li>
                    <li><strong>Horizontal scaling:</strong> Each layer scales independently (add more ingestion servers, Flink workers, etc.)</li>
                    <li><strong>Data retention tiers:</strong> Store raw data short-term, aggregates long-term (saves storage cost)</li>
                    <li><strong>Push model:</strong> Agents push metrics (simpler than pull, works behind NAT/firewalls)</li>
                </ul>
            </div>

            <h3>Step 3: Capacity Planning from Measured Results</h3>

            <div class="toggle-container" style="margin: 20px 0;">
                <button class="show-solution-btn" onclick="toggleArchitecture('phase0')" style="background: linear-gradient(135deg, #9b59b6 0%, #8e44ad 100%);">
                    📊 Show Capacity Calculation Flow
                </button>
            </div>

            <div id="phase0-before" class="architecture-flow active">
                <div class="command">$ ./build/load_test_persistent 8080 100 100 5000
Success Rate: 100.00%
Throughput: 2,253 RPS
Avg Latency: 0.25ms
Resource Usage: 45% CPU, 150MB memory</div>
            </div>

            <div id="phase0-after" class="architecture-flow" style="display: none;">
                <h4 style="color: #9b59b6; text-align: center; font-size: 1.3em; margin-bottom: 20px;">📊 Capacity Calculation Flow</h4>
                <div style="background: #2a2a2a; border-radius: 10px; padding: 25px; border: 2px solid #9b59b6;">

                    <div style="background: #1e1e1e; border: 2px solid #3498db; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #3498db; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #3498db; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">1</span>
                            Measure Current Performance
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">
                            <code style="background: #0a0a0a; padding: 2px 6px; border-radius: 3px;">2,253 RPS @ 45% CPU, 150MB memory</code>
                        </div>
                    </div>

                    <div style="text-align: center; color: #9b59b6; font-size: 1.5em; margin: 10px 0;">↓</div>

                    <div style="background: #1e1e1e; border: 2px solid #e67e22; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #e67e22; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #e67e22; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">2</span>
                            Estimate CPU Headroom (Conservative)
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">
                            Current: 2,253 RPS @ 45% CPU<br>
                            <span style="color: #f39c12;">⚠️ Naive calculation:</span> 2,253 × (90 / 45) = 5,000 RPS (assumes linear)<br>
                            <span style="color: #e67e22; font-weight: bold;">Realistic estimate:</span> 2,253 × 1.6 = <strong style="color: #e67e22;">~3,600 RPS</strong><br>
                            <span style="color: #7f8c8d; font-size: 0.85em;">(Accounting for context switching, cache misses, lock contention)</span>
                        </div>
                    </div>

                    <div style="text-align: center; color: #9b59b6; font-size: 1.5em; margin: 10px 0;">↓</div>

                    <div style="background: #1e1e1e; border: 2px solid #27ae60; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #27ae60; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #27ae60; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">3</span>
                            Calculate Horizontal Scaling
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">
                            Target: 100K metrics/sec<br>
                            Per-server capacity: 3,600 RPS (conservative)<br>
                            Servers needed (theory): 100K / 3.6K = 28 servers<br>
                            <strong style="color: #27ae60;">Production deployment: 35-40 servers</strong><br>
                            <span style="color: #7f8c8d; font-size: 0.85em;">(+25% overhead for load balancer, failures, hot partitions)</span>
                        </div>
                    </div>

                    <div style="background: #3a2a1f; border-left: 4px solid #f39c12; padding: 15px; margin-top: 20px; border-radius: 5px;">
                        <div style="color: #f39c12; font-weight: bold; margin-bottom: 8px;">⚠️ Real-World Reality:</div>
                        <div style="color: #95a5a6; font-size: 0.9em;">
                            • CPU scaling is <strong>never</strong> linear (context switching, cache misses increase)<br>
                            • Load balancers add 5-15% overhead<br>
                            • Hot partitions reduce effective capacity by 10-20%<br>
                            • Always provision 25-50% overhead for failures and degradation<br>
                            <br>
                            <strong style="color: #fff;">Lesson:</strong> Measure in production. Localhost tests are starting points, not guarantees.
                        </div>
                    </div>
                </div>
            </div>

            <div class="lab-section">
                <h4>🔬 Lab 0: Design Exercise</h4>
                <p><strong>Task:</strong> Design your message queue selection process</p>

                <ol>
                    <li>Compare Kafka vs RabbitMQ for 100K msg/sec throughput</li>
                    <li>Research actual benchmark numbers (not vendor claims)</li>
                    <li>Document trade-offs in a decision matrix</li>
                </ol>

                <h5>Expected Outcome</h5>
                <div class="success-box">
                    <p>A simple comparison table with:</p>
                    <ul>
                        <li>Measured throughput numbers</li>
                        <li>Operational complexity assessment</li>
                        <li>Your justified recommendation</li>
                    </ul>
                </div>
            </div>

        </div>

        <!-- We'll add Phase 1-7 content here in the next section -->
        <div class="content-panel" id="phase1">
            <h2>Phase 1: Thread-Per-Request Model</h2>
            <p style="font-size: 1.2em; color: #7f8c8d; margin-bottom: 30px;">
                <strong>Problem:</strong> Sequential request processing limits throughput to ~200 RPS<br>
                <strong>Solution:</strong> Spawn new thread for each incoming request<br>
                <strong>Result:</strong> 20 clients: 81% → 88% success rate
            </p>

            <div class="info-box">
                <h4>🎯 Learning Objectives</h4>
                <ul>
                    <li>Understand the difference between sequential and parallel request processing</li>
                    <li>Implement thread-per-request model using std::thread</li>
                    <li>Learn about thread detachment and lambda captures</li>
                    <li>Measure performance improvements with load testing</li>
                    <li>Identify when simple parallelism starts to break down</li>
                </ul>
            </div>

            <div class="lab-section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; margin: 25px 0;">
                <h4 style="color: #fff; margin-bottom: 15px;">🌿 Git Branches for Hands-On Practice</h4>
                <p style="color: #f0f0f0; margin-bottom: 15px;">We've created git branches so you can practice each phase progressively:</p>

                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin: 10px 0;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin-bottom: 10px;">$ git checkout phase-1-starter</div>
                    <p style="color: #fff; margin: 0; font-size: 0.9em;">📝 <strong>Starter code:</strong> Phase 0 complete (sequential) + TODOs for you to implement threading</p>
                </div>

                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin: 10px 0;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin-bottom: 10px;">$ git checkout phase-1-solution</div>
                    <p style="color: #fff; margin: 0; font-size: 0.9em;">✅ <strong>Reference solution:</strong> Complete Phase 1 implementation to compare against</p>
                </div>

                <p style="color: #f0f0f0; margin-top: 15px; font-size: 0.9em;"><strong>Recommended workflow:</strong></p>
                <ol style="color: #f0f0f0; font-size: 0.9em; margin-left: 20px;">
                    <li>Checkout <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px;">phase-1-starter</code></li>
                    <li>Read the TODO comments in <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px;">src/http_server.cpp</code></li>
                    <li>Implement threading following hints</li>
                    <li>Test with load_test and measure performance</li>
                    <li>Compare with <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px;">phase-1-solution</code> when done</li>
                </ol>
            </div>

            <h3>Architecture Evolution</h3>

            <div class="toggle-container" style="margin: 20px 0;">
                <button class="show-solution-btn" onclick="toggleArchitecture('phase1')" style="background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);">
                    🔄 Toggle: Before ↔ After
                </button>
            </div>

            <div id="phase1-before" class="architecture-flow active">
                <h4 style="color: #e74c3c; text-align: center; font-size: 1.3em; margin-bottom: 20px;">❌ Before: Sequential Processing</h4>
                <div style="background: #2a2a2a; border-radius: 10px; padding: 25px; border: 2px solid #e74c3c;">
                    <div style="background: #1e1e1e; border: 2px solid #3498db; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #3498db; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #3498db; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">1</span>
                            accept() - Wait for client connection
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">Server blocks here until client arrives</div>
                    </div>

                    <div style="text-align: center; color: #e74c3c; font-size: 1.5em; margin: 10px 0;">↓</div>

                    <div style="background: #1e1e1e; border: 2px solid #3498db; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #3498db; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #3498db; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">2</span>
                            HTTP parse (BLOCKING)
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">Parse request headers and body - all other clients wait</div>
                    </div>

                    <div style="text-align: center; color: #e74c3c; font-size: 1.5em; margin: 10px 0;">↓</div>

                    <div style="background: #1e1e1e; border: 2px solid #3498db; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #3498db; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #3498db; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">3</span>
                            File write (BLOCKING)
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">Write to disk - slowest operation, blocks everything</div>
                    </div>

                    <div style="text-align: center; color: #e74c3c; font-size: 1.5em; margin: 10px 0;">↓</div>

                    <div style="background: #1e1e1e; border: 2px solid #27ae60; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #27ae60; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #27ae60; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">4</span>
                            Response sent
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">Finally! Now we can accept the next client</div>
                    </div>

                    <div style="background: #3a1f1f; border-left: 4px solid #e74c3c; padding: 15px; margin-top: 20px; border-radius: 5px;">
                        <div style="color: #e74c3c; font-weight: bold; margin-bottom: 8px;">⚠️ The Problem:</div>
                        <div style="color: #95a5a6;">
                            • Head-of-line blocking: One slow request blocks all others<br>
                            • While processing Request A, Requests B, C, D wait in the kernel queue<br>
                            • Max throughput: ~200 RPS (limited by sequential execution)<br>
                            • Measured: 81% success rate @ 20 clients
                        </div>
                    </div>
                </div>
            </div>

            <div id="phase1-after" class="architecture-flow" style="display: none;">
                <h4 style="color: #27ae60; text-align: center; font-size: 1.3em; margin-bottom: 20px;">✅ After: Thread-Per-Request</h4>
                <div style="background: #2a2a2a; border-radius: 10px; padding: 25px; border: 2px solid #27ae60;">
                    <div style="background: #1e1e1e; border: 2px solid #3498db; border-radius: 8px; padding: 15px; margin: 10px 0;">
                        <div style="color: #3498db; font-weight: bold; margin-bottom: 8px;">
                            <span style="display: inline-block; background: #3498db; color: #fff; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; margin-right: 8px;">1</span>
                            accept() - Immediately spawn thread
                        </div>
                        <div style="color: #95a5a6; font-size: 0.9em;">
                            <code style="background: #0a0a0a; padding: 2px 6px; border-radius: 3px;">std::thread([this, client_socket]{ ... }).detach();</code>
                        </div>
                    </div>

                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin: 20px 0;">
                        <div style="background: #1f2a1f; border: 2px solid #27ae60; border-radius: 8px; padding: 12px;">
                            <div style="color: #27ae60; font-weight: bold; font-size: 0.9em; margin-bottom: 5px;">Thread 1</div>
                            <div style="color: #7f8c8d; font-size: 0.8em;">
                                Parse → Process → Write → Respond
                            </div>
                        </div>
                        <div style="background: #1f2a1f; border: 2px solid #27ae60; border-radius: 8px; padding: 12px;">
                            <div style="color: #27ae60; font-weight: bold; font-size: 0.9em; margin-bottom: 5px;">Thread 2</div>
                            <div style="color: #7f8c8d; font-size: 0.8em;">
                                Parse → Process → Write → Respond
                            </div>
                        </div>
                        <div style="background: #1f2a1f; border: 2px solid #27ae60; border-radius: 8px; padding: 12px;">
                            <div style="color: #27ae60; font-weight: bold; font-size: 0.9em; margin-bottom: 5px;">Thread 3</div>
                            <div style="color: #7f8c8d; font-size: 0.8em;">
                                Parse → Process → Write → Respond
                            </div>
                        </div>
                    </div>

                    <div style="background: #1f3a1f; border-left: 4px solid #27ae60; padding: 15px; margin-top: 20px; border-radius: 5px;">
                        <div style="color: #27ae60; font-weight: bold; margin-bottom: 8px;">✅ The Solution:</div>
                        <div style="color: #95a5a6;">
                            • Parallel execution: Multiple requests processed simultaneously<br>
                            • Main thread immediately returns to accept() after spawning worker<br>
                            • Each request gets its own thread with independent execution<br>
                            • Measured: 88% success rate @ 20 clients (+7% improvement)
                        </div>
                    </div>

                    <div style="background: #3a2a1f; border-left: 4px solid #f39c12; padding: 15px; margin-top: 15px; border-radius: 5px;">
                        <div style="color: #f39c12; font-weight: bold; margin-bottom: 8px;">⚡ Next Bottleneck Discovered:</div>
                        <div style="color: #95a5a6;">
                            File I/O is now the bottleneck - all threads compete for the file write mutex. This leads us to Phase 2: Async I/O.
                        </div>
                    </div>
                </div>
            </div>

            <div class="info-box" style="margin-top: 20px;">
                <h5>🎨 Additional Learning Resources</h5>
                <p><strong>Interactive Visualizations:</strong></p>
                <ul>
                    <li><a href="docs/metric_transformation_visual.html" target="_blank" style="color: #3498db; font-weight: bold;">📊 Metric Journey: User → Storage</a> - See how a metric transforms through all 7 stages (NEW!)</li>
                    <li><a href="docs/phase8_architecture_visual.html" target="_blank" style="color: #3498db; font-weight: bold;">⚡ Phase 8: Blocking vs Event-Driven</a> - 5-tab interactive comparison (NEW!)</li>
                    <li><a href="learning-resources/visualizations/request_flow_visual.html" target="_blank" style="color: #3498db; font-weight: bold;">Request Flow Visualization</a> - Phase 6 & 7 (Thread Pool + HTTP Keep-Alive)</li>
                    <li><a href="learning-resources/visualizations/event_driven_architecture.html" target="_blank" style="color: #3498db; font-weight: bold;">Event-Driven Architecture</a> - Compare with thread-based model</li>
                </ul>
                <p><strong>Deep Dive Concepts:</strong></p>
                <ul>
                    <li><a href="docs/end_to_end_flow.md" target="_blank" style="color: #3498db;">📖 End-to-End Flow Guide</a> - Complete technical documentation with Python, C++, SQL examples (NEW!)</li>
                    <li><a href="docs/blocking_io_explained.html" target="_blank" style="color: #3498db;">🔒 Blocking I/O Explained</a> - Why blocking wastes CPU (with animations!)</li>
                    <li><a href="learning-resources/concepts/connection_explained.txt" target="_blank" style="color: #3498db;">TCP Connection Mechanics</a> - Listen backlog & kernel queues</li>
                    <li><a href="learning-resources/concepts/queue_drain_explained.txt" target="_blank" style="color: #3498db;">Queue Draining Behavior</a> - Producer-consumer patterns</li>
                </ul>
            </div>

            <h3>📚 Background: Understanding Threading</h3>
            <div class="info-box">
                <h4>What is a Thread?</h4>
                <p>A thread is the smallest unit of execution within a process. Multiple threads share the same memory space but execute independently.</p>

                <h5>Key Concepts:</h5>
                <ul>
                    <li><strong>std::thread:</strong> C++11 thread class for creating new threads</li>
                    <li><strong>Lambda capture:</strong> [this, client_socket] captures variables for thread use</li>
                    <li><strong>detach():</strong> Allows thread to run independently from parent</li>
                    <li><strong>join():</strong> Waits for thread to complete (alternative to detach)</li>
                </ul>

                <h5>Why detach() instead of join()?</h5>
                <p>join() would block the accept loop until the request completes, defeating the purpose of threading. detach() allows the thread to run independently while we continue accepting new connections.</p>
            </div>

            <h3>🛠️ Hands-On Exercise</h3>
            <div class="lab-section">
                <h4>💻 Exercise 1.1: Implement Thread-Per-Request (25 minutes)</h4>

                <div class="warning-box">
                    <h5>⚠️ Before You Start</h5>
                    <p><strong>Prerequisite:</strong> You should have completed Phase 0 and understand the sequential baseline implementation.</p>
                    <p><strong>What you'll build:</strong> Transform the sequential HTTP server into a threaded version by implementing the TODO markers below.</p>
                </div>

                <h5>Step 1: Checkout Phase 1 Starter Branch</h5>
                <div class="lab-section" style="background: linear-gradient(135deg, #27ae60 0%, #229954 100%); border: none;">
                    <h5 style="color: #fff;">🌿 Ready to Code? Use the Git Branches!</h5>
                    <p style="color: rgba(255,255,255,0.95); line-height: 1.7;">We've created starter branches with TODO comments so you can implement Phase 1 yourself:</p>

                    <div class="command" style="background: #2c3e50; color: #ecf0f1; margin: 15px 0;">
# Start with Phase 1 starter (has Phase 0 baseline + TODOs)
$ git checkout phase-1-starter

# Build and see the sequential baseline
$ mkdir -p build
$ cmake -B build
$ make -C build metricstream_server load_test

# Run baseline test (you'll see poor performance)
$ ./build/metricstream_server &
$ ./build/load_test 8080 20 10
# Expected: ~60% success rate (sequential bottleneck)</div>

                    <p style="color: #fff; margin-top: 15px;"><strong>📝 Your Task:</strong> Open <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px;">src/http_server.cpp</code> and read the TODO comments. Implement thread-per-request pattern yourself!</p>

                    <p style="color: rgba(255,255,255,0.9); margin-top: 10px; font-size: 0.9em;">When done, compare your implementation with: <code style="background: rgba(0,0,0,0.3); padding: 2px 6px; border-radius: 3px;">git checkout phase-1-solution</code></p>
                </div>

                <h5>Step 2: Understand the Sequential Baseline</h5>
                <p>The <code>phase-1-starter</code> branch contains the Phase 0 sequential implementation. Here's what you're starting with:</p>

                <h5>Step 3: Sequential Baseline Code</h5>
                <div class="code-block"><pre><span class="comment">// src/http_server.cpp - PHASE 0: Sequential implementation (for reference)</span>

<span class="keyword">void</span> HttpServer::<span class="function">start</span>() {
    <span class="comment">// ... socket setup code above ...</span>

    <span class="keyword">while</span> (running_.load()) {
        <span class="comment">// Accept incoming connection</span>
        <span class="keyword">struct</span> sockaddr_in client_addr;
        socklen_t client_len = <span class="keyword">sizeof</span>(client_addr);

        <span class="keyword">int</span> client_socket = <span class="function">accept</span>(server_fd,
            (<span class="keyword">struct</span> sockaddr*)&client_addr, &client_len);

        <span class="keyword">if</span> (client_socket < 0) {
            <span class="keyword">continue</span>;  <span class="comment">// Error handling</span>
        }

        <span class="comment">// PHASE 0: Sequential processing - blocking!</span>
        <span class="comment">// This processes ONE request at a time</span>
        <span class="function">handle_client_request</span>(client_socket);
        <span class="function">close</span>(client_socket);
    }
}</pre></div>

                <h5>Step 4: Implement Thread-Per-Request (Your Turn!)</h5>
                <p><strong>This is where you code!</strong> Transform the sequential baseline into a threaded server. The TODO comments in <code>src/http_server.cpp</code> guide you through it.</p>

                <div class="info-box" style="background: #e8f5e9; border-left: 4px solid #27ae60;">
                    <p><strong>💡 Implementation Hints (from the TODO comments):</strong></p>
                    <ul>
                        <li>Create a lambda that captures <code>this</code> and <code>client_socket</code></li>
                        <li>Move all request processing (read, parse, handle, write, close) into the lambda</li>
                        <li>Spawn <code>std::thread</code> with the lambda</li>
                        <li>Call <code>.detach()</code> so the thread runs independently</li>
                        <li>The accept loop should ONLY accept and spawn threads—nothing else!</li>
                    </ul>
                    <p style="margin-top: 10px;"><strong>Expected result:</strong> Success rate improves from ~60% to ~85-90% with 20 clients</p>
                </div>

                <div class="code-block"><pre>
<span class="comment">// CONCEPT: Transform sequential code into thread-per-request model
//
// The transformation:
// 1. Create a new std::thread for each accepted connection
// 2. Move the handle_client_request() call into the thread
// 3. Capture 'this' and 'client_socket' in the lambda
// 4. Close the socket INSIDE the thread (not outside)
// 5. Detach the thread to allow independent execution
//
// Key insight:
// - Main thread continues accepting while worker threads process
// - Each request gets independent execution context
// - No head-of-line blocking
//</span>

<span class="keyword">while</span> (running_.load()) {
    <span class="keyword">int</span> client_socket = <span class="function">accept</span>(server_fd, ...);

    <span class="keyword">if</span> (client_socket < 0) {
        <span class="keyword">continue</span>;
    }

    <span class="comment">// TODO(student): Replace this line with thread creation ↓</span>
    <span class="function">handle_client_request</span>(client_socket);
    <span class="function">close</span>(client_socket);
    <span class="comment">// TODO(student): End of code to replace ↑</span>
}
                </pre></div>

                <h5>Step 5: Try It Yourself First!</h5>
                <div class="warning-box">
                    <p><strong>⚠️ Challenge Mode:</strong> Try implementing the solution yourself before looking at hints or solutions. You learn best by struggling with the problem first!</p>
                </div>

                <div class="hint-section">
                    <h5>💡 Hints (if you're stuck):</h5>
                    <ul>
                        <li>You need to use <code>std::thread</code> from the <code>&lt;thread&gt;</code> header</li>
                        <li>Lambda syntax: <code>[capture](params) { body }</code></li>
                        <li>Capture both <code>this</code> and <code>client_socket</code></li>
                        <li>Call <code>.detach()</code> on the thread to let it run independently</li>
                        <li>Move both <code>handle_client_request()</code> and <code>close()</code> inside the lambda</li>
                    </ul>
                </div>

                <button class="show-solution-btn" onclick="toggleSolution('solution-1-1')">
                    💡 Show Solution (try yourself first!)
                </button>

                <div id="solution-1-1" class="solution-section solution-hidden">
                    <h4>✅ Solution</h4>

                    <p><strong>Option A: Basic Thread Creation (Recommended)</strong></p>
                    <div class="code-block"><pre>
<span class="comment">// Simple approach - thread takes ownership</span>
std::<span class="function">thread</span>([<span class="keyword">this</span>, client_socket]() {
    <span class="function">handle_client_request</span>(client_socket);
    <span class="function">close</span>(client_socket);
}).<span class="function">detach</span>();
                    </pre></div>

                    <p><strong>Option B: Inline Request Handling (More Educational)</strong></p>
                    <div class="code-block"><pre>
<span class="comment">// Expanded version - handle request inline</span>
std::<span class="function">thread</span>([<span class="keyword">this</span>, client_socket]() {
    <span class="keyword">char</span> buffer[4096] = {0};
    ssize_t bytes_read = <span class="function">read</span>(client_socket, buffer, <span class="keyword">sizeof</span>(buffer) - 1);

    <span class="keyword">if</span> (bytes_read > 0) {
        std::string request_data(buffer, bytes_read);
        HttpRequest request = <span class="function">parse_request</span>(request_data);
        HttpResponse response = <span class="function">handle_request</span>(request);

        std::string response_str = <span class="function">format_response</span>(response);
        <span class="function">write</span>(client_socket, response_str.c_str(), response_str.length());
    }

    <span class="function">close</span>(client_socket);
}).<span class="function">detach</span>();
                    </pre></div>

                    <p><strong>Explanation:</strong></p>
                    <ul>
                        <li><code>[this, client_socket]</code> - Captures the server instance and socket for use in lambda</li>
                        <li><code>std::thread(...)</code> - Creates new thread with lambda as entry point</li>
                        <li><code>.detach()</code> - Allows thread to run independently; parent doesn't wait</li>
                        <li>Option A reuses existing <code>handle_client_request()</code> function</li>
                        <li>Option B expands the logic inline for more control</li>
                    </ul>

                    <p><strong>Which should you choose?</strong> Start with Option A (simpler). Once working, refactor to Option B to understand the full request flow.</p>

                    <div class="warning-box" style="margin-top: 20px; background: #2d1f1f; border-left: 4px solid #e74c3c;">
                        <h5 style="color: #e74c3c; margin-bottom: 10px;">🚨 Production Anti-Patterns - DO NOT USE IN REAL SYSTEMS</h5>
                        <p style="color: #ecf0f1;">This code demonstrates concurrency concepts but has <strong>critical flaws</strong> for production:</p>
                        <ul style="color: #bdc3c7;">
                            <li><strong>Unbounded thread creation:</strong> No limit on threads → can exhaust system resources under load</li>
                            <li><strong>.detach() without tracking:</strong> No way to know when threads finish or if they crash</li>
                            <li><strong>No graceful shutdown:</strong> Threads may still be running when server stops</li>
                            <li><strong>Missing error handling:</strong> No timeouts, connection limits, or resource cleanup</li>
                        </ul>
                        <p style="margin-top: 10px; color: #f39c12;"><strong>For production:</strong> Use thread pools (Phase 6), connection limits, timeouts, and proper lifecycle management. This code is for <em>learning</em>, not deployment.</p>
                    </div>
                </div>

                <h5>Step 6: Don't Forget the Include!</h5>
                <div class="code-block"><pre>
<span class="comment">// Add at the top of src/http_server.cpp</span>
<span class="keyword">#include</span> <span class="string">&lt;thread&gt;</span>
                </pre></div>

                <h5>Step 7: Build and Run the Current System</h5>
                <p><strong>Note:</strong> These commands build the complete Phase 7 system (all optimizations included).</p>
                <div class="command"># From the project root
$ mkdir -p build
$ cmake -B build
$ make -C build metricstream_server

# Run the server (in background or separate terminal)
$ ./build/metricstream_server

# Expected output:
Starting metrics ingestion server on port 8080
[ThreadPool] Started with 16 workers, max queue size: 10000
HTTP server started on port 8080
Ingestion service started</div>

                <h5>Step 8: Verify with curl (Single Request)</h5>
                <div class="command"># In a new terminal
$ curl -X POST http://localhost:8080/metrics \
  -H "Content-Type: application/json" \
  -H "Authorization: test_client" \
  -d '{"metrics":[{"timestamp":"2025-01-01T12:00:00Z","name":"cpu_usage","value":75.5,"type":"gauge"}]}'

# Expected output:
{"success":true,"metrics_processed":1}</div>

                <h5>Step 9: Load Test (Multiple Concurrent Requests)</h5>
                <div class="command"># Build the load test tool
$ make -C build load_test

# Run load test (20 clients, 10 requests each)
$ ./build/load_test 8080 20 10

# Expected output (Phase 7 with all optimizations):
Systems Craft Load Test
Target: 127.0.0.1:8080
Clients: 20
Requests per client: 10
Expected total requests: 200

=== Load Test Statistics ===
Duration: 1 seconds
Total Requests: 200
Successful: 200
Failed: 0
Success Rate: 100.00%
Requests/sec: 200.00
Avg Latency: 0.07 ms
=========================

# Note: Phase 1 (thread-per-request only) would show:
# Success Rate: 85-90% at this load
# Avg Latency: 0.8-1.5ms
# But you're running Phase 7, so you'll see 100% success!</div>

                <div class="success-box">
                    <h5>✅ Success Criteria</h5>
                    <ul>
                        <li>Server compiles without errors</li>
                        <li>Single curl request returns success</li>
                        <li>Load test shows <strong>80%+ success rate</strong> (vs ~50% sequential)</li>
                        <li>Metrics are written to <code>metrics.jsonl</code></li>
                    </ul>
                </div>
            </div>

            <div class="lab-section">
                <h4>💻 Exercise 1.2: Measure Thread Overhead (15 minutes)</h4>

                <p><strong>Goal:</strong> Understand when thread-per-request starts to break down.</p>

                <h5>Test Different Client Counts</h5>
                <div class="command"># Light load
$ ./build/load_test 8080 10 10

# Medium load
$ ./build/load_test 8080 20 10

# Heavy load
$ ./build/load_test 8080 50 10

# Extreme load
$ ./build/load_test 8080 100 10

# With Phase 7 optimizations, you should see:
# - 100% success rate at all levels
# - Latency increases slightly: 0.1ms → 0.5ms → 1.0ms → 1.5ms
# - System handles 100 concurrent clients without dropping requests</div>

                <h5>Record Your Results</h5>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Clients</th>
                            <th>Requests/Client</th>
                            <th>Your Success Rate</th>
                            <th>Expected Range</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>10</td>
                            <td>10</td>
                            <td><input type="text" style="width: 80px; padding: 5px;" placeholder="??%"></td>
                            <td>95-100%</td>
                        </tr>
                        <tr>
                            <td>20</td>
                            <td>10</td>
                            <td><input type="text" style="width: 80px; padding: 5px;" placeholder="??%"></td>
                            <td>85-90%</td>
                        </tr>
                        <tr>
                            <td>50</td>
                            <td>10</td>
                            <td><input type="text" style="width: 80px; padding: 5px;" placeholder="??%"></td>
                            <td>60-70%</td>
                        </tr>
                        <tr>
                            <td>100</td>
                            <td>10</td>
                            <td><input type="text" style="width: 80px; padding: 5px;" placeholder="??%"></td>
                            <td>40-50%</td>
                        </tr>
                    </tbody>
                </table>

                <div class="warning-box">
                    <h5>🤔 Reflection Questions</h5>
                    <ol>
                        <li>Why does success rate drop at higher client counts?</li>
                        <li>What system resource becomes the bottleneck?</li>
                        <li>How much overhead does thread creation add per request?</li>
                    </ol>
                    <p><strong>Hint:</strong> Each thread creation takes ~50-80μs of overhead, while the actual request work (parsing, rate limiting, queueing) takes ~150-200μs. Thread creation adds 30-40% overhead to each request!</p>
                </div>
            </div>

            <div class="lab-section">
                <h4>💻 Exercise 1.3: Thread Safety Bug Hunt (Optional, 20 minutes)</h4>

                <p><strong>Challenge:</strong> Find and fix a thread safety issue in the metrics counter.</p>

                <div class="code-block"><pre>
<span class="comment">// include/http_server.h - Thread-unsafe counter!</span>

<span class="keyword">class</span> HttpServer {
<span class="keyword">private</span>:
    <span class="keyword">int</span> total_requests_ = 0;  <span class="comment">// TODO(student): This is not thread-safe!</span>

<span class="keyword">public</span>:
    <span class="keyword">void</span> <span class="function">handle_request</span>(HttpRequest& req) {
        total_requests_++;  <span class="comment">// Race condition!</span>
        <span class="comment">// ... rest of handling ...</span>
    }
};
                </pre></div>

                <p><strong>Your Task:</strong> Fix the race condition using std::atomic.</p>

                <div class="hint-section">
                    <h5>💡 Thinking Points:</h5>
                    <ul>
                        <li>What happens when multiple threads increment <code>total_requests_</code> simultaneously?</li>
                        <li>Is <code>++</code> an atomic operation? (Hint: It's actually read-modify-write)</li>
                        <li>What are three ways to make this thread-safe?</li>
                    </ul>
                </div>

                <button class="show-solution-btn" onclick="toggleSolution('solution-1-3-hint')">
                    💡 Show Hint: Approaches to Consider
                </button>

                <div id="solution-1-3-hint" class="solution-section solution-hidden">
                    <h4>Three Possible Approaches</h4>

                    <div class="alternatives">
                        <div class="alternative chosen">
                            <h5>✅ std::atomic (Best)</h5>
                            <p>Use <code>std::atomic&lt;int&gt;</code> for lock-free increment</p>
                            <p><strong>Why:</strong> No locks, no contention, compiler handles safety</p>
                            <p><strong>Performance:</strong> ~10-20 CPU cycles</p>
                        </div>

                        <div class="alternative">
                            <h5>⚠️ std::mutex</h5>
                            <p>Protect counter with mutex lock</p>
                            <p><strong>Why not:</strong> Overkill for simple counter, adds contention</p>
                            <p><strong>Performance:</strong> ~100+ CPU cycles (lock overhead)</p>
                        </div>

                        <div class="alternative">
                            <h5>❌ Ignore it</h5>
                            <p>Hope the race condition doesn't matter</p>
                            <p><strong>Why not:</strong> Undefined behavior, incorrect metrics</p>
                            <p><strong>Result:</strong> Lost increments, wrong totals</p>
                        </div>
                    </div>
                </div>

                <button class="show-solution-btn" onclick="toggleSolution('solution-1-3')">
                    ✅ Show Solution
                </button>

                <div id="solution-1-3" class="solution-section solution-hidden">
                    <h4>✅ Solution: Using std::atomic</h4>

                    <div class="code-block"><pre>
<span class="comment">// include/http_server.h - Thread-safe counter with atomic</span>
<span class="keyword">#include</span> <span class="string">&lt;atomic&gt;</span>

<span class="keyword">class</span> HttpServer {
<span class="keyword">private</span>:
    std::atomic&lt;<span class="keyword">int</span>&gt; total_requests_{0};  <span class="comment">// ✅ Thread-safe!</span>

<span class="keyword">public</span>:
    <span class="keyword">void</span> <span class="function">handle_request</span>(HttpRequest& req) {
        <span class="comment">// Option 1: Simple increment (uses default memory ordering)</span>
        total_requests_++;

        <span class="comment">// Option 2: Explicit fetch_add (more control over memory ordering)</span>
        <span class="comment">// total_requests_.fetch_add(1, std::memory_order_relaxed);</span>

        <span class="comment">// ... rest of handling ...</span>
    }

    <span class="keyword">int</span> <span class="function">get_total_requests</span>() <span class="keyword">const</span> {
        <span class="keyword">return</span> total_requests_.load(std::memory_order_relaxed);
    }
};
                    </pre></div>

                    <p><strong>Explanation:</strong></p>
                    <ul>
                        <li><code>std::atomic&lt;int&gt;</code> provides lock-free, thread-safe operations</li>
                        <li>Increment (<code>++</code>) is now atomic - no race condition</li>
                        <li><code>memory_order_relaxed</code> is sufficient for simple counters (no ordering requirements)</li>
                        <li>Compiler generates appropriate CPU instructions (e.g., <code>lock inc</code> on x86)</li>
                    </ul>

                    <p><strong>Why atomic is better than mutex for counters:</strong></p>
                    <ul>
                        <li>No context switching overhead</li>
                        <li>No deadlock risk</li>
                        <li>Hardware-level atomic instructions (lock-free)</li>
                        <li>Significantly faster for simple increment/decrement</li>
                    </ul>
                </div>
            </div>

            <h3>📊 Performance Analysis</h3>
            <div class="success-box">
                <h4>Expected Results Summary</h4>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Sequential (Phase 0)</th>
                            <th>Thread-Per-Request (Phase 1)</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1 client</td>
                            <td>100%</td>
                            <td>100%</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>20 clients</td>
                            <td>50%</td>
                            <td><span class="improvement">88%</span></td>
                            <td><span class="improvement">+38pp</span></td>
                        </tr>
                        <tr>
                            <td>50 clients</td>
                            <td>30%</td>
                            <td><span class="improvement">65%</span></td>
                            <td><span class="improvement">+35pp</span></td>
                        </tr>
                        <tr>
                            <td>Throughput</td>
                            <td>~200 RPS</td>
                            <td>~800 RPS</td>
                            <td><span class="improvement">4x</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="decision-point">
                <h4>🤔 Design Decision: Why Thread-Per-Request?</h4>

                <div class="alternatives">
                    <div class="alternative">
                        <h5>❌ Sequential Processing</h5>
                        <p><strong>Pros:</strong> Simple, no concurrency bugs</p>
                        <p><strong>Cons:</strong> 200 RPS max, head-of-line blocking</p>
                        <p><strong>Verdict:</strong> Too slow for production</p>
                    </div>

                    <div class="alternative chosen">
                        <h5>✅ Thread-Per-Request</h5>
                        <p><strong>Pros:</strong> Simple parallelism, immediate gains</p>
                        <p><strong>Cons:</strong> Thread creation overhead (500μs)</p>
                        <p><strong>Verdict:</strong> Best for initial optimization</p>
                    </div>

                    <div class="alternative">
                        <h5>⏭️ Thread Pool</h5>
                        <p><strong>Pros:</strong> Amortizes creation cost</p>
                        <p><strong>Cons:</strong> More complex, premature optimization</p>
                        <p><strong>Verdict:</strong> Save for Phase 6</p>
                    </div>
                </div>
            </div>

            <div class="warning-box">
                <h4>⚠️ Key Learning: Thread Creation Overhead</h4>
                <p>Measured end-to-end latency: <strong>~250μs per request</strong> (0.25ms average). Profiling shows thread creation adds ~50-80μs overhead per request, which is 30-40% of total processing time. At high concurrency (100+ threads), this overhead becomes the bottleneck.</p>
                <p style="margin-top: 10px; color: #7f8c8d; font-size: 0.9em;">
                    <strong>Breakdown:</strong> Thread creation (~60μs) + JSON parsing (~80μs) + Rate limiting check (~20μs) + Queue write (~40μs) + HTTP response (~50μs) = ~250μs total
                </p>
            </div>

            <div class="success-box">
                <h4>✅ What You Learned</h4>
                <ul>
                    <li>How to use std::thread for parallelism in C++</li>
                    <li>The performance impact of thread-per-request model</li>
                    <li>How to measure success rates and latency</li>
                    <li>Why simple solutions are best for initial optimizations</li>
                </ul>
            </div>

            <h3 style="margin-top: 50px;">🚀 Remaining Phases in Milestone 1</h3>
            <p style="color: #7f8c8d; font-size: 1.1em; margin-bottom: 30px;">
                Phase 1 established parallelism but introduced new bottlenecks. Phases 2-7 systematically optimize each bottleneck discovered through profiling and measurement.
            </p>

        </div>

        <!-- Phase 2 Panel -->
        <div class="content-panel" id="phase2">
            <h2>Phase 2: Async I/O with Producer-Consumer Pattern</h2>

            <div class="info-box" style="background: #1a252f; border-left: 4px solid #3498db;">
                <h4 style="color: #3498db;">Learning Objectives</h4>
                <ul>
                    <li>Understand the I/O blocking problem in threaded servers</li>
                    <li>Implement producer-consumer pattern with condition variables</li>
                    <li>Learn queue draining mechanics and backpressure handling</li>
                    <li>Measure I/O bottleneck impact on overall throughput</li>
                </ul>
            </div>

            <div class="lab-section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; margin: 25px 0;">
                <h4 style="color: #fff; margin-bottom: 15px;">🌿 Git Branches for Phase 2</h4>

                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin: 10px 0;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin-bottom: 10px;">$ git checkout phase-2-starter</div>
                    <p style="color: #fff; margin: 0; font-size: 0.9em;">📝 <strong>Starter:</strong> Phase 1 complete (threading) - ready for you to add async I/O</p>
                </div>

                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 15px; margin: 10px 0;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin-bottom: 10px;">$ git checkout phase-2-solution</div>
                    <p style="color: #fff; margin: 0; font-size: 0.9em;">✅ <strong>Solution:</strong> Complete async I/O with producer-consumer queue</p>
                </div>

                <p style="color: #f0f0f0; margin-top: 15px; font-size: 0.9em;"><strong>What to implement:</strong> Background writer thread, task queue, condition variables for synchronization</p>
            </div>

            <h4 style="margin-top: 30px;">The Problem: File I/O Blocking</h4>
            <div class="warning-box">
                <p><strong>Bottleneck Discovered:</strong> After Phase 1, profiling revealed that each request thread blocks for ~5ms waiting for file I/O to complete. With 50 concurrent threads, this serializes writes and creates contention.</p>

                <div class="code-block" style="margin-top: 15px;"><pre><span class="comment">// Phase 1 implementation - BLOCKING I/O</span>

<span class="keyword">void</span> <span class="function">handle_request</span>(HttpRequest& request) {
    <span class="comment">// Parse, validate, rate limit... (~200μs)</span>
    MetricBatch batch = <span class="function">parse_json</span>(request.body);

    <span class="comment">// THIS BLOCKS THE THREAD FOR 5ms! ⚠️</span>
    <span class="function">write_to_file</span>(batch);  <span class="comment">// Expensive disk I/O</span>

    <span class="comment">// Send response</span>
    <span class="function">send_response</span>(client_socket, <span class="string">"success"</span>);
}

<span class="comment">// Result: Thread waits 5ms just to write to disk
// With 50 threads, file I/O becomes serialized
// Success rate: 59% at 50 clients</span></pre></div>

                <p style="margin-top: 15px; color: #e74c3c;"><strong>Why this is bad:</strong> Request processing takes ~200μs, but we block for 5ms on I/O. That's 25x more time spent waiting than working!</p>
            </div>

            <h4 style="margin-top: 30px;">The Solution: Producer-Consumer Queue</h4>
            <div class="info-box">
                <p><strong>Strategy:</strong> Separate I/O from request handling. Request threads become "producers" that queue metrics, while a dedicated "consumer" thread handles all file writes asynchronously.</p>

                <div style="display: flex; gap: 20px; margin-top: 20px;">
                    <div style="flex: 1; background: #2d1f1f; padding: 15px; border-radius: 8px; border-left: 4px solid #e74c3c;">
                        <h5 style="color: #e74c3c; margin-bottom: 10px;">Before (Synchronous)</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Thread 1: Parse → Write (5ms) → Respond
Thread 2: Parse → Wait → Write (5ms) → Respond
Thread 3: Parse → Wait → Wait → Write (5ms)

Bottleneck: File I/O serialized</pre>
                    </div>
                    <div style="flex: 1; background: #1f2d1f; padding: 15px; border-radius: 8px; border-left: 4px solid #27ae60;">
                        <h5 style="color: #27ae60; margin-bottom: 10px;">After (Asynchronous)</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Thread 1: Parse → Queue (1μs) → Respond ✅
Thread 2: Parse → Queue (1μs) → Respond ✅
Thread 3: Parse → Queue (1μs) → Respond ✅

I/O Thread: Write batch (5ms) → Write batch...</pre>
                    </div>
                </div>
            </div>

            <h4 style="margin-top: 30px;">Implementation: Condition Variables</h4>
            <div class="code-block"><pre><span class="comment">// src/ingestion_service.cpp - Producer side (request threads)</span>

<span class="keyword">void</span> IngestionService::<span class="function">queue_metrics_for_async_write</span>(<span class="keyword">const</span> MetricBatch& batch) {
    {
        std::lock_guard&lt;std::mutex&gt; lock(queue_mutex_);
        write_queue_.push(batch);  <span class="comment">// Add to queue</span>
    }
    queue_cv_.<span class="function">notify_one</span>();  <span class="comment">// Wake up writer thread</span>
}

<span class="comment">// Consumer side (dedicated I/O thread)</span>
<span class="keyword">void</span> IngestionService::<span class="function">async_writer_loop</span>() {
    <span class="keyword">while</span> (writer_running_) {
        std::unique_lock&lt;std::mutex&gt; lock(queue_mutex_);

        <span class="comment">// Wait for work or shutdown signal</span>
        queue_cv_.<span class="function">wait</span>(lock, [<span class="keyword">this</span>] {
            <span class="keyword">return</span> !write_queue_.empty() || !writer_running_;
        });

        <span class="comment">// Process all pending batches</span>
        <span class="keyword">while</span> (!write_queue_.empty() && writer_running_) {
            MetricBatch batch = write_queue_.front();
            write_queue_.pop();

            <span class="comment">// Release lock BEFORE expensive I/O! ⚡</span>
            lock.<span class="function">unlock</span>();

            <span class="function">store_metrics_to_file</span>(batch);  <span class="comment">// 5ms, but doesn't block request threads</span>

            <span class="comment">// Reacquire lock for next iteration</span>
            lock.<span class="function">lock</span>();
        }
    }
}</pre></div>

            <div class="info-box" style="margin-top: 20px; background: #1a1f2e; border-left: 4px solid #3498db;">
                <h5 style="color: #3498db;">Key Insight: Lock Minimization</h5>
                <p>Notice how we <code>unlock()</code> before the expensive I/O operation. This is critical:</p>
                <ul>
                    <li><strong>Lock held:</strong> Only during queue manipulation (~1μs)</li>
                    <li><strong>Lock released:</strong> During file write (5ms)</li>
                    <li><strong>Benefit:</strong> Request threads can queue new metrics while I/O happens</li>
                </ul>
                <p style="margin-top: 10px; color: #7f8c8d; font-style: italic;">This pattern is called "lock splitting" - minimize critical section to only what needs protection.</p>
            </div>

            <h4 style="margin-top: 30px;">Performance Measurement</h4>
            <div class="success-box">
                <h5>Before and After Comparison</h5>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Phase 1 (Blocking I/O)</th>
                            <th>Phase 2 (Async I/O)</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Success Rate @ 50 clients</td>
                            <td>59%</td>
                            <td><span class="improvement">66%</span></td>
                            <td><span class="improvement">+7pp</span></td>
                        </tr>
                        <tr>
                            <td>Avg Request Latency</td>
                            <td>~5.2ms</td>
                            <td><span class="improvement">~1.8ms</span></td>
                            <td><span class="improvement">3.4ms faster</span></td>
                        </tr>
                        <tr>
                            <td>Thread Block Time</td>
                            <td>5ms (I/O wait)</td>
                            <td><span class="improvement">~1μs (queue)</span></td>
                            <td><span class="improvement">5000x faster</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="warning-box" style="margin-top: 20px;">
                <h5>Why Only +7% Success Rate?</h5>
                <p>Async I/O helped, but we're still limited by other bottlenecks:</p>
                <ul>
                    <li><strong>JSON parsing:</strong> Still using slow O(n²) string concatenation</li>
                    <li><strong>Rate limiting mutex:</strong> Single global lock causes contention</li>
                    <li><strong>Thread creation:</strong> 60μs overhead per request</li>
                </ul>
                <p style="margin-top: 10px; color: #f39c12;"><strong>Lesson:</strong> Optimization is iterative. Fix one bottleneck, measure, find the next one.</p>
            </div>

            <div class="success-box" style="margin-top: 30px;">
                <h4>What You Learned</h4>
                <ul>
                    <li><strong>Producer-Consumer Pattern:</strong> Separate fast operations from slow ones</li>
                    <li><strong>Condition Variables:</strong> Efficient thread coordination (no busy-waiting)</li>
                    <li><strong>Lock Splitting:</strong> Release locks before expensive operations</li>
                    <li><strong>Queue Draining:</strong> How accept loops and task queues manage backpressure</li>
                </ul>
            </div>

            <h4 style="margin-top: 20px;">Additional Resources</h4>
            <div class="info-box">
                <ul>
                    <li><a href="learning-resources/concepts/queue_drain_explained.txt" target="_blank" style="color: #3498db;">Queue Draining Explained</a> - Deep dive into kernel listen queues vs application queues</li>
                </ul>
            </div>

        </div>

        <!-- Phase 3 Panel -->
        <div class="content-panel" id="phase3">
            <h2>Phase 3: JSON Parser Optimization</h2>

            <div class="info-box" style="background: #1a252f; border-left: 4px solid #e67e22;">
                <h4 style="color: #e67e22;">Learning Objectives</h4>
                <ul>
                    <li>Identify algorithmic complexity bottlenecks through profiling</li>
                    <li>Understand O(n²) vs O(n) string operations</li>
                    <li>Implement single-pass parsing with state machines</li>
                    <li>Learn memory allocation patterns and cache-friendly code</li>
                </ul>
            </div>

            <div class="lab-section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; margin: 25px 0;">
                <h4 style="color: #fff; margin-bottom: 10px;">🌿 Git Branches</h4>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-3-starter</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Phase 2 code + space for optimization</p>
                </div>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-3-solution</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Optimized JSON parser implementation</p>
                </div>
            </div>

            <h4 style="margin-top: 30px;">The Problem: O(n²) String Concatenation</h4>
            <div class="warning-box">
                <p><strong>Profiling Result:</strong> JSON parsing takes 80μs per request - far too slow! Investigation revealed repeated string reallocations.</p>

                <div class="code-block" style="margin-top: 15px;"><pre><span class="comment">// Phase 2 implementation - O(n²) complexity ⚠️</span>

MetricBatch IngestionService::<span class="function">parse_json_metrics</span>(<span class="keyword">const</span> std::string& json_body) {
    <span class="comment">// Find metrics array</span>
    size_t metrics_pos = json_body.<span class="function">find</span>(<span class="string">"\"metrics\""</span>);
    size_t array_start = json_body.<span class="function">find</span>(<span class="string">"["</span>, metrics_pos);
    size_t array_end = json_body.<span class="function">find</span>(<span class="string">"]"</span>, array_start);

    <span class="comment">// ⚠️ substr() allocates new string (copy entire substring)</span>
    std::string metrics_array = json_body.<span class="function">substr</span>(array_start + 1, array_end - array_start - 1);

    <span class="comment">// Parse each metric object</span>
    size_t pos = 0;
    <span class="keyword">while</span> (pos < metrics_array.length()) {
        size_t obj_start = metrics_array.<span class="function">find</span>(<span class="string">"{"</span>, pos);
        size_t obj_end = metrics_array.<span class="function">find</span>(<span class="string">"}"</span>, obj_start);

        <span class="comment">// ⚠️ Another allocation! Copies substring again</span>
        std::string metric_obj = metrics_array.<span class="function">substr</span>(obj_start, obj_end - obj_start + 1);

        <span class="comment">// ⚠️ And another for each field extraction!</span>
        std::string name = <span class="function">extract_string_field</span>(metric_obj, <span class="string">"name"</span>);

        pos = obj_end + 1;
    }
}

<span class="comment">// Complexity: O(n²) - creates n substrings, each copying O(n) characters
// Memory: Allocates ~10+ strings per metric
// Time: ~80μs per request (30% of total processing time!)</span></pre></div>

                <p style="margin-top: 15px; color: #e74c3c;"><strong>The Hidden Cost:</strong> Each <code>substr()</code> allocates a new string and copies data. For a 500-byte JSON payload, this creates 10+ allocations and copies 2-3KB of data!</p>
            </div>

            <h4 style="margin-top: 30px;">The Solution: Single-Pass State Machine Parser</h4>
            <div class="info-box">
                <p><strong>Strategy:</strong> Parse JSON in a single pass with no string allocations. Use state machine to track position and extract fields directly.</p>

                <div style="display: flex; gap: 20px; margin-top: 20px;">
                    <div style="flex: 1; background: #2d1f1f; padding: 15px; border-radius: 8px; border-left: 4px solid #e74c3c;">
                        <h5 style="color: #e74c3c; margin-bottom: 10px;">Old: Multiple Passes</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Pass 1: Find "metrics" → substr()
Pass 2: Find each "{" → substr()
Pass 3: Extract "name" → substr()
Pass 4: Extract "value" → substr()
...

Allocations: 10+ per metric
Time: O(n²)</pre>
                    </div>
                    <div style="flex: 1; background: #1f2d1f; padding: 15px; border-radius: 8px; border-left: 4px solid #27ae60;">
                        <h5 style="color: #27ae60; margin-bottom: 10px;">New: Single Pass</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">One loop: i = 0 → n
  See '"' → parse_string(result)
  See '0-9' → parse_number()
  See '{' → enter object state
  See '}' → exit object state

Allocations: 2 pre-allocated buffers
Time: O(n)</pre>
                    </div>
                </div>
            </div>

            <h4 style="margin-top: 30px;">Implementation: State Machine Parser</h4>
            <div class="code-block"><pre><span class="comment">// src/ingestion_service.cpp - O(n) single-pass parser</span>

MetricBatch IngestionService::<span class="function">parse_json_metrics_optimized</span>(<span class="keyword">const</span> std::string& json_body) {
    MetricBatch batch;

    <span class="comment">// State machine for parsing</span>
    <span class="keyword">enum class</span> ParseState {
        LOOKING_FOR_METRICS, IN_METRICS_ARRAY, IN_METRIC_OBJECT,
        PARSING_FIELD_NAME, PARSING_STRING_VALUE, PARSING_NUMBER_VALUE
    };

    ParseState state = ParseState::LOOKING_FOR_METRICS;
    size_t i = 0;
    <span class="keyword">const</span> size_t len = json_body.length();

    <span class="comment">// ⚡ Pre-allocated buffers (no allocations during parse!)</span>
    std::string current_field, current_value;
    current_field.<span class="function">reserve</span>(32);
    current_value.<span class="function">reserve</span>(128);

    <span class="comment">// Current metric being built</span>
    std::string metric_name, metric_type = <span class="string">"gauge"</span>;
    <span class="keyword">double</span> metric_value = 0.0;

    <span class="comment">// Lambda helpers (inline, no function call overhead)</span>
    <span class="keyword">auto</span> skip_whitespace = [&]() {
        <span class="keyword">while</span> (i < len && std::isspace(json_body[i])) i++;
    };

    <span class="keyword">auto</span> parse_string = [&](std::string& result) {
        result.<span class="function">clear</span>();  <span class="comment">// Reuse buffer, no allocation</span>
        <span class="keyword">if</span> (i >= len || json_body[i] != <span class="string">'"'</span>) <span class="keyword">return false</span>;
        i++;  <span class="comment">// skip opening quote</span>

        <span class="keyword">while</span> (i < len && json_body[i] != <span class="string">'"'</span>) {
            result += json_body[i++];  <span class="comment">// Direct char append</span>
        }
        <span class="keyword">if</span> (i < len && json_body[i] == <span class="string">'"'</span>) {
            i++;  <span class="comment">// skip closing quote</span>
            <span class="keyword">return true</span>;
        }
        <span class="keyword">return false</span>;
    };

    <span class="keyword">auto</span> parse_number = [&]() -> <span class="keyword">double</span> {
        size_t start = i;
        <span class="keyword">while</span> (i < len && (std::isdigit(json_body[i]) || json_body[i] == <span class="string">'.'</span>)) i++;

        <span class="comment">// ⚡ Use pointer arithmetic - no string allocation!</span>
        <span class="keyword">const char</span>* start_ptr = json_body.<span class="function">data</span>() + start;
        <span class="keyword">const char</span>* end_ptr = json_body.<span class="function">data</span>() + i;
        <span class="keyword">return</span> std::strtod(start_ptr, <span class="keyword">const_cast</span>&lt;<span class="keyword">char</span>**&gt;(&end_ptr));
    };

    <span class="comment">// ⚡ Single pass through entire JSON (O(n))</span>
    <span class="keyword">while</span> (i < len) {
        skip_whitespace();
        <span class="comment">// ... state machine logic ...</span>
    }

    <span class="keyword">return</span> batch;
}</pre></div>

            <div class="info-box" style="margin-top: 20px; background: #1a1f2e; border-left: 4px solid #e67e22;">
                <h5 style="color: #e67e22;">Key Insight: Zero-Allocation Parsing</h5>
                <p>The optimized parser achieves O(n) complexity through three techniques:</p>
                <ul>
                    <li><strong>Buffer reuse:</strong> <code>current_field.clear()</code> instead of creating new strings</li>
                    <li><strong>Pointer arithmetic:</strong> <code>json_body.data() + i</code> for zero-copy number parsing</li>
                    <li><strong>Single pass:</strong> One loop from start to end, state machine tracks context</li>
                </ul>
                <p style="margin-top: 10px; color: #7f8c8d; font-style: italic;">Allocations dropped from 10+ per metric to 2 total (pre-allocated buffers).</p>
            </div>

            <h4 style="margin-top: 30px;">Performance Measurement</h4>
            <div class="success-box">
                <h5>Before and After Comparison</h5>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Phase 2 (O(n²) Parser)</th>
                            <th>Phase 3 (O(n) Parser)</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Success Rate @ 100 clients</td>
                            <td>~40%</td>
                            <td><span class="improvement">80.2%</span></td>
                            <td><span class="improvement">+40pp</span></td>
                        </tr>
                        <tr>
                            <td>Parsing Time per Request</td>
                            <td>~80μs</td>
                            <td><span class="improvement">~15μs</span></td>
                            <td><span class="improvement">5.3x faster</span></td>
                        </tr>
                        <tr>
                            <td>Avg Latency</td>
                            <td>~4.5ms</td>
                            <td><span class="improvement">2.73ms</span></td>
                            <td><span class="improvement">1.77ms faster</span></td>
                        </tr>
                        <tr>
                            <td>Memory Allocations per Metric</td>
                            <td>10-15</td>
                            <td><span class="improvement">0 (reuse buffers)</span></td>
                            <td><span class="improvement">100% reduction</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="success-box" style="margin-top: 30px;">
                <h4>What You Learned</h4>
                <ul>
                    <li><strong>Algorithmic Complexity:</strong> O(n²) vs O(n) makes huge difference at scale</li>
                    <li><strong>String Allocation Costs:</strong> substr() and string concatenation are expensive</li>
                    <li><strong>State Machine Parsing:</strong> Single-pass algorithms for efficiency</li>
                    <li><strong>Profiling-Driven Optimization:</strong> Measure first, optimize hot paths</li>
                </ul>
            </div>

        </div>

        <!-- Phase 4 Panel -->
        <div class="content-panel" id="phase4">
            <h2>Phase 4: Hash-Based Mutex Pool</h2>

            <div class="info-box" style="background: #1a252f; border-left: 4px solid #9b59b6;">
                <h4 style="color: #9b59b6;">Learning Objectives</h4>
                <ul>
                    <li>Understand lock contention and its performance impact</li>
                    <li>Learn lock granularity strategies (coarse vs fine-grained)</li>
                    <li>Implement hash-based sharding for mutex pools</li>
                    <li>Measure lock contention reduction through profiling</li>
                </ul>
            </div>

            <div class="lab-section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; margin: 25px 0;">
                <h4 style="color: #fff; margin-bottom: 10px;">🌿 Git Branches</h4>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-4-starter</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Phase 3 code + single global mutex</p>
                </div>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-4-solution</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Hash-based mutex pool (10K mutexes)</p>
                </div>
            </div>

            <h4 style="margin-top: 30px;">The Problem: Global Mutex Contention</h4>
            <div class="warning-box">
                <p><strong>Bottleneck:</strong> Rate limiting uses a single global mutex. With 50+ concurrent threads, all threads compete for the same lock, causing severe contention.</p>

                <div class="code-block" style="margin-top: 15px;"><pre><span class="comment">// Phase 3 - Single global mutex ⚠️</span>

<span class="keyword">class</span> RateLimiter {
<span class="keyword">private</span>:
    std::mutex global_mutex_;  <span class="comment">// ⚠️ All threads compete for this!</span>
    std::unordered_map&lt;std::string, ClientQueue&gt; client_requests_;

<span class="keyword">public</span>:
    <span class="keyword">bool</span> <span class="function">allow_request</span>(<span class="keyword">const</span> std::string& client_id) {
        std::lock_guard&lt;std::mutex&gt; lock(global_mutex_);  <span class="comment">// Bottleneck!</span>

        <span class="comment">// Rate limiting logic for THIS client
        // But we lock ALL clients unnecessarily!</span>
        <span class="keyword">auto</span>& queue = client_requests_[client_id];
        <span class="comment">// ... check rate limit ...</span>

        <span class="keyword">return</span> allowed;
    }
};

<span class="comment">// Problem: Client A and Client B don't share state,
// yet they both wait for the same mutex!</span></pre></div>

                <p style="margin-top: 15px; color: #e74c3c;"><strong>The Waste:</strong> Thread processing client_id="A" blocks thread processing client_id="B", even though their data is independent!</p>
            </div>

            <h4 style="margin-top: 30px;">The Solution: Hash-Based Mutex Pool</h4>
            <div class="info-box">
                <p><strong>Strategy:</strong> Use 16 separate mutexes. Hash each client_id to one of the 16 mutexes. Different clients likely get different mutexes → less contention.</p>

                <div style="display: flex; gap: 20px; margin-top: 20px;">
                    <div style="flex: 1; background: #2d1f1f; padding: 15px; border-radius: 8px; border-left: 4px solid #e74c3c;">
                        <h5 style="color: #e74c3c; margin-bottom: 10px;">Before: Global Mutex</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Thread 1 (client_A): Lock global → work
Thread 2 (client_B): ⏳ Wait for lock
Thread 3 (client_C): ⏳ Wait for lock
Thread 4 (client_D): ⏳ Wait for lock

Contention: 100% (all threads blocked)</pre>
                    </div>
                    <div style="flex: 1; background: #1f2d1f; padding: 15px; border-radius: 8px; border-left: 4px solid #27ae60;">
                        <h5 style="color: #27ae60; margin-bottom: 10px;">After: Mutex Pool (16 mutexes)</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Thread 1 (client_A): Lock mutex[5] → work ✅
Thread 2 (client_B): Lock mutex[12] → work ✅
Thread 3 (client_C): Lock mutex[3] → work ✅
Thread 4 (client_A2): ⏳ Wait for mutex[5]

Contention: ~6% (only same-hash collisions)</pre>
                    </div>
                </div>
            </div>

            <h4 style="margin-top: 30px;">Implementation: Hash Function Sharding</h4>
            <div class="code-block"><pre><span class="comment">// src/ingestion_service.cpp - Hash-based mutex pool</span>

<span class="keyword">class</span> RateLimiter {
<span class="keyword">private</span>:
    <span class="keyword">static constexpr</span> size_t MUTEX_POOL_SIZE = 16;  <span class="comment">// Power of 2 for fast modulo</span>
    std::mutex client_mutex_pool_[MUTEX_POOL_SIZE];  <span class="comment">// Array of mutexes</span>

    <span class="comment">// Hash client_id to select mutex</span>
    std::mutex& <span class="function">get_client_mutex</span>(<span class="keyword">const</span> std::string& client_id) {
        std::hash&lt;std::string&gt; hasher;
        size_t hash_value = hasher(client_id);
        size_t mutex_index = hash_value % MUTEX_POOL_SIZE;  <span class="comment">// Map to [0, 15]</span>
        <span class="keyword">return</span> client_mutex_pool_[mutex_index];
    }

<span class="keyword">public</span>:
    <span class="keyword">bool</span> <span class="function">allow_request</span>(<span class="keyword">const</span> std::string& client_id) {
        <span class="comment">// ⚡ Lock only the mutex for THIS client's hash bucket</span>
        std::mutex& client_lock = <span class="function">get_client_mutex</span>(client_id);
        std::lock_guard&lt;std::mutex&gt; lock(client_lock);

        <span class="comment">// Rate limiting logic (same as before)
        // But now only blocks threads with same hash!</span>
        <span class="keyword">auto</span>& queue = client_requests_[client_id];
        <span class="comment">// ... check rate limit ...</span>

        <span class="keyword">return</span> allowed;
    }
};</pre></div>

            <div class="info-box" style="margin-top: 20px; background: #1a1f2e; border-left: 4px solid #9b59b6;">
                <h5 style="color: #9b59b6;">Key Insight: Lock Granularity Trade-offs</h5>
                <p>Three levels of lock granularity, from coarse to fine:</p>
                <ul>
                    <li><strong>Global mutex (Phase 3):</strong> Simple, but maximum contention</li>
                    <li><strong>Mutex pool (Phase 4):</strong> Good balance - 16x less contention, minimal overhead</li>
                    <li><strong>Per-client mutex:</strong> Zero contention, but high memory (1 mutex per client)</li>
                </ul>
                <p style="margin-top: 10px; color: #7f8c8d; font-style: italic;">Mutex pool gives 90%+ benefit of per-client with only 16 mutexes (vs thousands).</p>
            </div>

            <div class="warning-box" style="margin-top: 20px;">
                <h5>Why 16 Mutexes?</h5>
                <p><strong>Trade-off analysis:</strong></p>
                <ul>
                    <li><strong>Too few (2-4):</strong> Still high contention, only 2-4x improvement</li>
                    <li><strong>Sweet spot (16-32):</strong> Contention reduced to <10%, minimal memory</li>
                    <li><strong>Too many (1000+):</strong> Diminishing returns, cache pollution, memory waste</li>
                </ul>
                <p style="margin-top: 10px; color: #f39c12;"><strong>Rule of thumb:</strong> Mutex pool size ≈ 2 × number of cores, or 16 for general use.</p>
            </div>

            <h4 style="margin-top: 30px;">Performance Measurement</h4>
            <div class="success-box">
                <h5>Lock Contention Reduction</h5>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Phase 3 (Global Mutex)</th>
                            <th>Phase 4 (Mutex Pool)</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Lock Wait Time (avg)</td>
                            <td>~50μs per request</td>
                            <td><span class="improvement">~3μs</span></td>
                            <td><span class="improvement">16x faster</span></td>
                        </tr>
                        <tr>
                            <td>Contention Rate @ 100 threads</td>
                            <td>95% (almost all block)</td>
                            <td><span class="improvement">~6%</span></td>
                            <td><span class="improvement">16x reduction</span></td>
                        </tr>
                        <tr>
                            <td>Throughput Impact</td>
                            <td>Baseline</td>
                            <td><span class="improvement">+15-20%</span></td>
                            <td><span class="improvement">Better CPU utilization</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="success-box" style="margin-top: 30px;">
                <h4>What You Learned</h4>
                <ul>
                    <li><strong>Lock Contention:</strong> Multiple threads competing for same lock = performance killer</li>
                    <li><strong>Hash-Based Sharding:</strong> Distribute load across multiple locks using hash function</li>
                    <li><strong>Lock Granularity:</strong> Trade-off between simplicity (coarse) and performance (fine)</li>
                    <li><strong>Mutex Pool Pattern:</strong> Industry-standard technique for high-concurrency systems</li>
                </ul>
            </div>

        </div>

        <!-- Phase 5 Panel -->
        <div class="content-panel" id="phase5">
            <h2>Phase 5: Thread Pool Architecture</h2>

            <div class="info-box" style="background: #1a252f; border-left: 4px solid #27ae60;">
                <h4 style="color: #27ae60;">Learning Objectives</h4>
                <ul>
                    <li>Understand thread creation overhead and its impact at scale</li>
                    <li>Implement worker pool pattern with task queue</li>
                    <li>Learn thread lifecycle management and graceful shutdown</li>
                    <li>Achieve 100% success rate through proper resource pooling</li>
                </ul>
            </div>

            <div class="lab-section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; margin: 25px 0;">
                <h4 style="color: #fff; margin-bottom: 10px;">🌿 Git Branches</h4>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-5-starter</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Phase 4 code with thread-per-request</p>
                </div>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-5-solution</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Thread pool with worker queue</p>
                </div>
            </div>

            <h4 style="margin-top: 30px;">The Problem: Thread Creation Overhead</h4>
            <div class="warning-box">
                <p><strong>Profiling Discovery:</strong> Creating a new thread takes ~60μs. With 200μs total per request, thread creation is 30% of our processing time!</p>

                <div class="code-block" style="margin-top: 15px;"><pre><span class="comment">// Phase 1-4: Thread-per-request model ⚠️</span>

<span class="keyword">while</span> (running_) {
    <span class="keyword">int</span> client_socket = <span class="function">accept</span>(server_fd, ...);

    <span class="comment">// ⚠️ Create new thread for EVERY request (expensive!)</span>
    std::thread([<span class="keyword">this</span>, client_socket]() {
        <span class="function">handle_request</span>(client_socket);  <span class="comment">// ~200μs work</span>
        <span class="function">close</span>(client_socket);
    }).<span class="function">detach</span>();
}

<span class="comment">// Cost breakdown per request:
// - Thread creation: ~60μs (kernel allocates stack, sets up TLS, etc.)
// - Actual work: ~140μs (parse, rate limit, queue, respond)
// - Thread destruction: ~20μs (cleanup)
// Total: ~220μs, but 80μs is just thread overhead!</span></pre></div>

                <p style="margin-top: 15px; color: #e74c3c;"><strong>The Problem:</strong> We're spending more time creating/destroying threads than doing actual work. This limits throughput and wastes CPU cycles.</p>
            </div>

            <h4 style="margin-top: 30px;">The Solution: Worker Pool with Task Queue</h4>
            <div class="info-box">
                <p><strong>Strategy:</strong> Pre-create 8 worker threads at startup. When requests arrive, queue them as tasks. Workers pull tasks from queue and execute them. Amortize thread creation cost across all requests.</p>

                <div style="display: flex; gap: 20px; margin-top: 20px;">
                    <div style="flex: 1; background: #2d1f1f; padding: 15px; border-radius: 8px; border-left: 4px solid #e74c3c;">
                        <h5 style="color: #e74c3c; margin-bottom: 10px;">Before: Thread Per Request</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Request 1: Create thread → Work → Destroy
Request 2: Create thread → Work → Destroy
Request 3: Create thread → Work → Destroy
...

Overhead: 60μs creation + 20μs destroy
           = 80μs per request (36%!)</pre>
                    </div>
                    <div style="flex: 1; background: #1f2d1f; padding: 15px; border-radius: 8px; border-left: 4px solid #27ae60;">
                        <h5 style="color: #27ae60; margin-bottom: 10px;">After: Thread Pool</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Startup: Create 8 threads (one-time cost)

Request 1: Queue task → Worker picks up → Work
Request 2: Queue task → Worker picks up → Work
Request 3: Queue task → Worker picks up → Work

Overhead: ~1μs to queue task (99% reduction!)</pre>
                    </div>
                </div>
            </div>

            <h4 style="margin-top: 30px;">Implementation: ThreadPool Class</h4>
            <div class="code-block"><pre><span class="comment">// src/thread_pool.cpp - Worker pool implementation</span>

<span class="keyword">class</span> ThreadPool {
<span class="keyword">private</span>:
    std::vector&lt;std::thread&gt; workers_;           <span class="comment">// Pre-created threads</span>
    std::queue&lt;std::function&lt;<span class="keyword">void</span>()&gt;&gt; tasks_;  <span class="comment">// Task queue</span>
    std::mutex queue_mutex_;
    std::condition_variable condition_;
    std::atomic&lt;<span class="keyword">bool</span>&gt; stop_{<span class="keyword">false</span>};

<span class="keyword">public</span>:
    <span class="comment">// Constructor: Pre-create worker threads</span>
    <span class="function">ThreadPool</span>(size_t num_threads) {
        workers_.<span class="function">reserve</span>(num_threads);
        <span class="keyword">for</span> (size_t i = 0; i < num_threads; ++i) {
            workers_.<span class="function">emplace_back</span>(&ThreadPool::worker_loop, <span class="keyword">this</span>);
        }
    }

    <span class="comment">// Add task to queue (called from accept loop)</span>
    <span class="keyword">bool</span> <span class="function">enqueue</span>(std::function&lt;<span class="keyword">void</span>()&gt; task) {
        {
            std::lock_guard&lt;std::mutex&gt; lock(queue_mutex_);
            tasks_.<span class="function">push</span>(std::move(task));  <span class="comment">// ~1μs</span>
        }
        condition_.<span class="function">notify_one</span>();  <span class="comment">// Wake up one worker</span>
        <span class="keyword">return true</span>;
    }

    <span class="comment">// Worker loop (runs in each pre-created thread)</span>
    <span class="keyword">void</span> <span class="function">worker_loop</span>() {
        <span class="keyword">while</span> (<span class="keyword">true</span>) {
            std::function&lt;<span class="keyword">void</span>()&gt; task;

            {
                std::unique_lock&lt;std::mutex&gt; lock(queue_mutex_);

                <span class="comment">// Wait for task or shutdown signal</span>
                condition_.<span class="function">wait</span>(lock, [<span class="keyword">this</span>] {
                    <span class="keyword">return</span> stop_.load() || !tasks_.empty();
                });

                <span class="keyword">if</span> (stop_.load() && tasks_.empty()) <span class="keyword">return</span>;  <span class="comment">// Shutdown</span>

                <span class="comment">// Get task from queue</span>
                task = std::move(tasks_.front());
                tasks_.pop();
            }

            <span class="comment">// Execute task outside lock (prevents blocking other workers)</span>
            <span class="keyword">if</span> (task) {
                task();  <span class="comment">// Do the actual work</span>
            }
        }
    }
};</pre></div>

            <h4 style="margin-top: 30px;">Integration: Accept Loop with Thread Pool</h4>
            <div class="code-block"><pre><span class="comment">// src/http_server.cpp - Using thread pool</span>

<span class="keyword">void</span> HttpServer::<span class="function">start</span>() {
    <span class="comment">// Create thread pool at startup (one-time cost)</span>
    ThreadPool pool(8);  <span class="comment">// 8 worker threads</span>

    <span class="keyword">while</span> (running_) {
        <span class="keyword">int</span> client_socket = <span class="function">accept</span>(server_fd, ...);

        <span class="comment">// ⚡ Queue task instead of creating thread (1μs vs 60μs!)</span>
        pool.<span class="function">enqueue</span>([<span class="keyword">this</span>, client_socket]() {
            <span class="function">handle_request</span>(client_socket);
            <span class="function">close</span>(client_socket);
        });
    }
}</pre></div>

            <div class="info-box" style="margin-top: 20px; background: #1a1f2e; border-left: 4px solid #27ae60;">
                <h5 style="color: #27ae60;">Key Insight: Queue Draining Rate</h5>
                <p>The accept loop now drains the kernel listen queue at maximum speed:</p>
                <ul>
                    <li><strong>Old (thread-per-request):</strong> accept() → create thread (60μs) → accept() next → ~16,000/sec</li>
                    <li><strong>New (thread pool):</strong> accept() → queue task (1μs) → accept() next → ~1,000,000/sec</li>
                </ul>
                <p style="margin-top: 10px; color: #7f8c8d; font-style: italic;">The kernel queue never fills up because we drain it 60x faster. This eliminates "connection refused" errors.</p>
            </div>

            <h4 style="margin-top: 30px;">Performance Measurement</h4>
            <div class="success-box">
                <h5>Thread Pool Impact - Breakthrough Result!</h5>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Phase 4 (Thread-Per-Request)</th>
                            <th>Phase 5 (Thread Pool)</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Success Rate @ 100 clients</td>
                            <td>80.2%</td>
                            <td><span class="improvement">100%</span></td>
                            <td><span class="improvement">+19.8pp 🎉</span></td>
                        </tr>
                        <tr>
                            <td>Avg Latency</td>
                            <td>2.73ms</td>
                            <td><span class="improvement">0.65ms</span></td>
                            <td><span class="improvement">4.2x faster</span></td>
                        </tr>
                        <tr>
                            <td>Thread Overhead</td>
                            <td>80μs per request</td>
                            <td><span class="improvement">~1μs (amortized)</span></td>
                            <td><span class="improvement">80x reduction</span></td>
                        </tr>
                        <tr>
                            <td>Throughput</td>
                            <td>~800 RPS</td>
                            <td><span class="improvement">~15,000 RPS</span></td>
                            <td><span class="improvement">18.7x increase</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="warning-box" style="margin-top: 20px; background: #2d1f1f; border-left: 4px solid #e74c3c;">
                <h5 style="color: #e74c3c;">🚨 Production Anti-Patterns - Thread Pool Edition</h5>
                <p>Our thread pool demonstrates core concepts but lacks production features:</p>
                <ul>
                    <li><strong>No backpressure:</strong> Unbounded queue can exhaust memory under extreme load</li>
                    <li><strong>No priority queues:</strong> All tasks treated equally (important requests can't jump ahead)</li>
                    <li><strong>No work stealing:</strong> Idle workers can't help busy ones (load imbalance)</li>
                    <li><strong>Fixed size:</strong> Can't dynamically scale workers based on load</li>
                </ul>
                <p style="margin-top: 10px; color: #f39c12;"><strong>For production:</strong> Use libraries like Intel TBB, Folly ThreadPool, or std::jthread (C++20) with proper queue limits and monitoring.</p>
            </div>

            <div class="success-box" style="margin-top: 30px;">
                <h4>What You Learned</h4>
                <ul>
                    <li><strong>Thread Pool Pattern:</strong> Amortize thread creation cost across all requests</li>
                    <li><strong>Worker Queue:</strong> Decouple task submission from execution</li>
                    <li><strong>Condition Variables:</strong> Efficient worker wakeup (no busy-waiting)</li>
                    <li><strong>Lifecycle Management:</strong> Graceful shutdown with join() and stop flags</li>
                </ul>
            </div>

            <h4 style="margin-top: 20px;">Additional Resources</h4>
            <div class="info-box">
                <ul>
                    <li><a href="learning-resources/visualizations/request_flow_visual.html" target="_blank" style="color: #27ae60;">Thread Pool Visualization</a> - Interactive diagram of request flow through worker pool</li>
                </ul>
            </div>

        </div>

        <!-- Phase 6 Panel -->
        <div class="content-panel" id="phase6">
            <h2>Phase 6: Lock-Free Ring Buffer</h2>

            <div class="info-box" style="background: #1a252f; border-left: 4px solid #e74c3c;">
                <h4 style="color: #e74c3c;">Learning Objectives</h4>
                <ul>
                    <li>Understand lock-free data structures and their benefits</li>
                    <li>Learn atomic operations and memory ordering semantics</li>
                    <li>Implement single-writer/single-reader lock-free ring buffer</li>
                    <li>Master std::memory_order for performance and correctness</li>
                </ul>
            </div>

            <div class="lab-section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; margin: 25px 0;">
                <h4 style="color: #fff; margin-bottom: 10px;">🌿 Git Branches</h4>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-6-starter</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Phase 5 code with mutex-based metrics</p>
                </div>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-6-solution</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Lock-free ring buffer with atomics</p>
                </div>
            </div>

            <h4 style="margin-top: 30px;">The Problem: Metrics Collection Mutex</h4>
            <div class="warning-box">
                <p><strong>Hidden Overhead:</strong> Phase 5 achieved 100% success, but profiling shows metrics collection (for monitoring) adds mutex overhead to every request.</p>

                <div class="code-block" style="margin-top: 15px;"><pre><span class="comment">// Phase 5: Mutex-protected metrics collection</span>

<span class="keyword">struct</span> ClientMetrics {
    std::mutex mutex;  <span class="comment">// ⚠️ Lock for every metric event</span>
    std::vector&lt;MetricEvent&gt; events;
};

<span class="keyword">bool</span> RateLimiter::<span class="function">allow_request</span>(<span class="keyword">const</span> std::string& client_id) {
    <span class="comment">// ... rate limiting logic ...</span>

    <span class="comment">// Record metrics (for monitoring/debugging)</span>
    {
        std::lock_guard&lt;std::mutex&gt; lock(client_metrics_[client_id].mutex);
        client_metrics_[client_id].events.<span class="function">push_back</span>({now, decision});
    }  <span class="comment">// ⚠️ Adds 10-20μs per request!</span>

    <span class="keyword">return</span> decision;
}

<span class="comment">// Problem: Metrics collection is NOT on the critical path
// (it's for debugging/monitoring), yet it slows down EVERY request</span></pre></div>

                <p style="margin-top: 15px; color: #e74c3c;"><strong>The Irony:</strong> We're adding overhead to measure overhead! Metrics collection should be nearly free.</p>
            </div>

            <h4 style="margin-top: 30px;">The Solution: Lock-Free Ring Buffer</h4>
            <div class="info-box">
                <p><strong>Strategy:</strong> Use lock-free ring buffer with atomic index. Single writer (request thread) and single reader (flush thread) can operate without locks using memory ordering guarantees.</p>

                <div style="display: flex; gap: 20px; margin-top: 20px;">
                    <div style="flex: 1; background: #2d1f1f; padding: 15px; border-radius: 8px; border-left: 4px solid #e74c3c;">
                        <h5 style="color: #e74c3c; margin-bottom: 10px;">Before: Mutex-Protected Vector</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Writer: Lock → push_back() → Unlock
Reader: Lock → read events → Unlock

Cost: 10-20μs per write (mutex + allocation)
Contention: If reader locks, writer blocks</pre>
                    </div>
                    <div style="flex: 1; background: #1f2d1f; padding: 15px; border-radius: 8px; border-left: 4px solid #27ae60;">
                        <h5 style="color: #27ae60; margin-bottom: 10px;">After: Lock-Free Ring Buffer</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Writer: buf[idx] = event; idx.store(+1)
Reader: read_idx → idx.load(); read buf

Cost: ~2-5ns (atomic store, no syscall!)
Contention: None (single writer/reader)</pre>
                    </div>
                </div>
            </div>

            <h4 style="margin-top: 30px;">Implementation: Atomic Ring Buffer</h4>
            <div class="code-block"><pre><span class="comment">// src/ingestion_service.cpp - Lock-free metrics collection</span>

<span class="keyword">struct</span> ClientMetrics {
    <span class="keyword">static constexpr</span> size_t BUFFER_SIZE = 1024;

    <span class="comment">// Ring buffer (fixed-size array, no allocations)</span>
    MetricEvent ring_buffer[BUFFER_SIZE];

    <span class="comment">// Atomic indices (lock-free synchronization)</span>
    std::atomic&lt;size_t&gt; write_index{0};  <span class="comment">// Writer updates this</span>
    std::atomic&lt;size_t&gt; read_index{0};   <span class="comment">// Reader updates this</span>
};

<span class="comment">// WRITER: Request thread (called on every request)</span>
<span class="keyword">bool</span> RateLimiter::<span class="function">allow_request</span>(<span class="keyword">const</span> std::string& client_id) {
    <span class="comment">// ... rate limiting logic ...</span>

    <span class="comment">// ⚡ Lock-free metrics write (single-writer pattern)</span>
    <span class="keyword">auto</span>& metrics = client_metrics_[client_id];
    size_t write_idx = metrics.write_index.<span class="function">load</span>(std::memory_order_relaxed);

    <span class="comment">// Write event to ring buffer (no lock needed!)</span>
    metrics.ring_buffer[write_idx % ClientMetrics::BUFFER_SIZE] = MetricEvent{now, decision};

    <span class="comment">// Publish write with release semantics
    // Ensures buffer write visible before index update</span>
    metrics.write_index.<span class="function">store</span>(write_idx + 1, std::memory_order_release);

    <span class="keyword">return</span> decision;
}

<span class="comment">// READER: Flush thread (called periodically, not per-request)</span>
<span class="keyword">void</span> RateLimiter::<span class="function">flush_metrics</span>() {
    <span class="keyword">for</span> (<span class="keyword">auto</span>& [client_id, metrics] : client_metrics_) {
        size_t read_idx = metrics.read_index.<span class="function">load</span>(std::memory_order_relaxed);
        size_t write_idx = metrics.write_index.<span class="function">load</span>(std::memory_order_acquire);

        <span class="comment">// Read all events written since last flush</span>
        <span class="keyword">while</span> (read_idx < write_idx) {
            MetricEvent event = metrics.ring_buffer[read_idx % ClientMetrics::BUFFER_SIZE];
            <span class="comment">// ... process event (log, aggregate, etc.) ...</span>
            read_idx++;
        }

        <span class="comment">// Update read index</span>
        metrics.read_index.<span class="function">store</span>(read_idx, std::memory_order_release);
    }
}</pre></div>

            <div class="info-box" style="margin-top: 20px; background: #1a1f2e; border-left: 4px solid #e74c3c;">
                <h5 style="color: #e74c3c;">Key Insight: Memory Ordering Semantics</h5>
                <p>The ring buffer uses three memory orderings for correctness and performance:</p>
                <ul>
                    <li><strong>relaxed (load write_index):</strong> Just need current value, no ordering required</li>
                    <li><strong>release (store write_index):</strong> Ensures buffer write completes before index visible</li>
                    <li><strong>acquire (load write_index in reader):</strong> Ensures we see buffer write after seeing index</li>
                </ul>
                <p style="margin-top: 10px; color: #7f8c8d; font-style: italic;">This forms a release-acquire pair: writer "releases" data, reader "acquires" it with proper visibility.</p>
            </div>

            <div class="warning-box" style="margin-top: 20px;">
                <h5>Why Ring Buffer vs Unbounded Queue?</h5>
                <p><strong>Trade-off:</strong> Ring buffer is fixed-size and can overwrite old data. Why use it?</p>
                <ul>
                    <li><strong>No allocations:</strong> Fixed-size array, no malloc/free per event</li>
                    <li><strong>Cache-friendly:</strong> Sequential memory access, fits in CPU cache</li>
                    <li><strong>Bounded memory:</strong> Can't exhaust memory (unbounded queue can)</li>
                    <li><strong>Lock-free:</strong> Single writer/reader pattern with atomics (no mutex)</li>
                </ul>
                <p style="margin-top: 10px; color: #f39c12;"><strong>Acceptable loss:</strong> For metrics/debugging, losing old events under extreme load is fine. Correctness over completeness.</p>
            </div>

            <h4 style="margin-top: 30px;">Performance Measurement</h4>
            <div class="success-box">
                <h5>Lock-Free Metrics Impact</h5>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Phase 5 (Mutex)</th>
                            <th>Phase 6 (Lock-Free)</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Metrics Collection Overhead</td>
                            <td>10-20μs per request</td>
                            <td><span class="improvement">~5ns</span></td>
                            <td><span class="improvement">2000-4000x faster</span></td>
                        </tr>
                        <tr>
                            <td>Success Rate @ 100 clients</td>
                            <td>100%</td>
                            <td><span class="improvement">100%</span></td>
                            <td>Maintained ✅</td>
                        </tr>
                        <tr>
                            <td>Avg Latency</td>
                            <td>0.65ms</td>
                            <td><span class="improvement">0.63ms</span></td>
                            <td><span class="improvement">20μs faster</span></td>
                        </tr>
                        <tr>
                            <td>CPU Utilization</td>
                            <td>Baseline</td>
                            <td><span class="improvement">-3%</span></td>
                            <td><span class="improvement">Less lock overhead</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="success-box" style="margin-top: 30px;">
                <h4>What You Learned</h4>
                <ul>
                    <li><strong>Lock-Free Data Structures:</strong> Atomics instead of mutexes for coordination</li>
                    <li><strong>Memory Ordering:</strong> relaxed, acquire, release for correctness and performance</li>
                    <li><strong>Ring Buffer Pattern:</strong> Fixed-size circular buffer for bounded memory</li>
                    <li><strong>Single-Writer/Single-Reader:</strong> Simplest lock-free pattern (no ABA problem)</li>
                </ul>
            </div>

        </div>

        <!-- Phase 7 Panel -->
        <div class="content-panel" id="phase7">
            <h2>Phase 7: HTTP Keep-Alive & Listen Backlog Tuning</h2>

            <div class="info-box" style="background: #1a252f; border-left: 4px solid #f39c12;">
                <h4 style="color: #f39c12;">Learning Objectives</h4>
                <ul>
                    <li>Understand TCP connection establishment overhead</li>
                    <li>Implement HTTP/1.1 persistent connections (Keep-Alive)</li>
                    <li>Learn kernel listen queue mechanics and backlog tuning</li>
                    <li>Achieve production-ready throughput (2000+ RPS)</li>
                </ul>
            </div>

            <div class="lab-section" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; margin: 25px 0;">
                <h4 style="color: #fff; margin-bottom: 10px;">🌿 Git Branches</h4>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-7-starter</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">Phase 6 code, one conn per request</p>
                </div>
                <div style="background: rgba(255,255,255,0.1); border-radius: 8px; padding: 12px; margin: 8px 0; display: flex; align-items: center; justify-content: space-between;">
                    <div class="command" style="background: rgba(0,0,0,0.3); margin: 0; flex: 0 0 220px;">git checkout phase-7-solution</div>
                    <p style="color: #fff; margin: 0; font-size: 0.85em;">HTTP Keep-Alive + 10K listen backlog (2,253 RPS!)</p>
                </div>
            </div>

            <h4 style="margin-top: 30px;">The Problem: TCP Handshake Overhead</h4>
            <div class="warning-box">
                <p><strong>Final Bottleneck:</strong> Even with thread pool, each request creates a new TCP connection. The three-way handshake (SYN → SYN-ACK → ACK) adds latency and limits throughput.</p>

                <div class="code-block" style="margin-top: 15px;"><pre><span class="comment">// Phase 6: One connection per request ⚠️</span>

<span class="comment">// Client side (load test)</span>
<span class="keyword">for</span> (<span class="keyword">int</span> i = 0; i < num_requests; i++) {
    <span class="keyword">int</span> sock = <span class="function">socket</span>(AF_INET, SOCK_STREAM, 0);
    <span class="function">connect</span>(sock, ...);  <span class="comment">// ⚠️ TCP handshake: SYN → SYN-ACK → ACK (~0.1-0.5ms)</span>

    <span class="function">send</span>(sock, request);
    <span class="function">recv</span>(sock, response);

    <span class="function">close</span>(sock);  <span class="comment">// ⚠️ Teardown: FIN → ACK → FIN → ACK</span>
}

<span class="comment">// Server side (accept loop)</span>
<span class="keyword">while</span> (<span class="keyword">true</span>) {
    <span class="keyword">int</span> client = <span class="function">accept</span>(server_fd, ...);  <span class="comment">// New connection per request</span>
    pool.<span class="function">enqueue</span>([client]() {
        <span class="function">handle_request</span>(client);
        <span class="function">close</span>(client);  <span class="comment">// Close after ONE request ⚠️</span>
    });
}

<span class="comment">// Cost: ~0.3-0.5ms for connection setup/teardown
// That's MORE than our actual processing time (0.25ms)!</span></pre></div>

                <p style="margin-top: 15px; color: #e74c3c;"><strong>The Waste:</strong> Connection overhead is now larger than processing time. We've optimized the server, but TCP handshake is the bottleneck!</p>
            </div>

            <h4 style="margin-top: 30px;">The Solution: HTTP Keep-Alive + Backlog Tuning</h4>
            <div class="info-box">
                <p><strong>Two-Part Strategy:</strong></p>
                <ol>
                    <li><strong>Keep-Alive:</strong> Reuse connections for multiple requests (amortize handshake cost)</li>
                    <li><strong>Listen Backlog:</strong> Increase kernel queue size to handle connection bursts</li>
                </ol>

                <div style="display: flex; gap: 20px; margin-top: 20px;">
                    <div style="flex: 1; background: #2d1f1f; padding: 15px; border-radius: 8px; border-left: 4px solid #e74c3c;">
                        <h5 style="color: #e74c3c; margin-bottom: 10px;">Before: New Connection Per Request</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Client: connect() → request → close()
        connect() → request → close()
        connect() → request → close()

Cost: 0.5ms × 100 requests = 50ms
Backlog: 10 (kernel rejects burst > 10)</pre>
                    </div>
                    <div style="flex: 1; background: #1f2d1f; padding: 15px; border-radius: 8px; border-left: 4px solid #27ae60;">
                        <h5 style="color: #27ae60; margin-bottom: 10px;">After: Keep-Alive + Backlog 1024</h5>
                        <pre style="color: #ecf0f1; font-size: 0.85em;">Client: connect() → request → request → ...
                      → request (100x) → close()

Cost: 0.5ms once + (0.25ms × 100) = 25.5ms
Backlog: 1024 (handles 100 simultaneous)</pre>
                    </div>
                </div>
            </div>

            <h4 style="margin-top: 30px;">Implementation: Persistent Connection Loop</h4>
            <div class="code-block"><pre><span class="comment">// src/http_server.cpp - Keep-Alive implementation</span>

<span class="comment">// Server setup: Increase listen backlog</span>
<span class="keyword">int</span> server_fd = <span class="function">socket</span>(AF_INET, SOCK_STREAM, 0);
<span class="function">bind</span>(server_fd, ...);
<span class="function">listen</span>(server_fd, 1024);  <span class="comment">// ⚡ Was 10, now 1024 (100x larger kernel queue!)</span>

<span class="comment">// Worker thread: Handle multiple requests per connection</span>
<span class="keyword">void</span> <span class="function">handle_client_connection</span>(<span class="keyword">int</span> client_socket) {
    <span class="comment">// Set socket timeout (60 seconds idle timeout)</span>
    <span class="keyword">struct</span> timeval timeout;
    timeout.tv_sec = 60;
    timeout.tv_usec = 0;
    <span class="function">setsockopt</span>(client_socket, SOL_SOCKET, SO_RCVTIMEO, &timeout, <span class="keyword">sizeof</span>(timeout));

    <span class="keyword">bool</span> keep_alive = <span class="keyword">true</span>;

    <span class="comment">// ⚡ Loop: Handle multiple requests on same connection</span>
    <span class="keyword">while</span> (keep_alive) {
        <span class="comment">// Read HTTP request</span>
        <span class="keyword">char</span> buffer[4096];
        ssize_t bytes = <span class="function">read</span>(client_socket, buffer, <span class="keyword">sizeof</span>(buffer));

        <span class="keyword">if</span> (bytes <= 0) <span class="keyword">break</span>;  <span class="comment">// Connection closed or error</span>

        <span class="comment">// Parse request headers</span>
        HttpRequest req = <span class="function">parse_request</span>(buffer, bytes);

        <span class="comment">// Check if client wants to close connection</span>
        <span class="keyword">if</span> (req.headers[<span class="string">"Connection"</span>] == <span class="string">"close"</span>) {
            keep_alive = <span class="keyword">false</span>;
        }

        <span class="comment">// Process request (rate limit, parse JSON, queue metrics)</span>
        HttpResponse res = <span class="function">handle_request</span>(req);

        <span class="comment">// ⚡ Add Keep-Alive header to response</span>
        <span class="keyword">if</span> (keep_alive) {
            res.headers[<span class="string">"Connection"</span>] = <span class="string">"keep-alive"</span>;
            res.headers[<span class="string">"Keep-Alive"</span>] = <span class="string">"timeout=60"</span>;
        } <span class="keyword">else</span> {
            res.headers[<span class="string">"Connection"</span>] = <span class="string">"close"</span>;
        }

        <span class="comment">// Send response</span>
        std::string response_str = <span class="function">format_response</span>(res);
        <span class="function">write</span>(client_socket, response_str.c_str(), response_str.length());
    }

    <span class="function">close</span>(client_socket);  <span class="comment">// Close after ALL requests on this connection</span>
}</pre></div>

            <div class="info-box" style="margin-top: 20px; background: #1a1f2e; border-left: 4px solid #f39c12;">
                <h5 style="color: #f39c12;">Key Insight: Listen Backlog Impact</h5>
                <p>The kernel listen queue (backlog) is critical for handling connection bursts:</p>
                <ul>
                    <li><strong>Backlog = 10:</strong> Kernel can queue 10 pending connections. 11th client gets "Connection refused"</li>
                    <li><strong>Backlog = 1024:</strong> Kernel can queue 1024 pending connections. Absorbs bursts easily</li>
                    <li><strong>Why it matters:</strong> 100 clients connecting simultaneously → backlog=10 rejects 90, backlog=1024 accepts all</li>
                </ul>
                <p style="margin-top: 10px; color: #7f8c8d; font-style: italic;">The backlog size should be >= max expected concurrent connection attempts.</p>
            </div>

            <h4 style="margin-top: 30px;">Performance Measurement</h4>
            <div class="success-box">
                <h5>Keep-Alive + Backlog Tuning - Production Ready!</h5>
                <table class="metrics-table">
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>Phase 6 (New Conn/Req)</th>
                            <th>Phase 7 (Keep-Alive)</th>
                            <th>Improvement</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Success Rate @ 100 clients</td>
                            <td>46-51%</td>
                            <td><span class="improvement">100%</span></td>
                            <td><span class="improvement">+49-54pp 🎉</span></td>
                        </tr>
                        <tr>
                            <td>Throughput (sustained)</td>
                            <td>~1,000 RPS</td>
                            <td><span class="improvement">2,253 RPS</span></td>
                            <td><span class="improvement">2.25x increase</span></td>
                        </tr>
                        <tr>
                            <td>Per-Request Latency</td>
                            <td>2.05ms (w/ handshake)</td>
                            <td><span class="improvement">0.25ms (reused conn)</span></td>
                            <td><span class="improvement">8.2x faster</span></td>
                        </tr>
                        <tr>
                            <td>Connection Overhead</td>
                            <td>0.5ms × 10,000 = 5s</td>
                            <td><span class="improvement">0.5ms × 100 = 50ms</span></td>
                            <td><span class="improvement">99% reduction</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="warning-box" style="margin-top: 20px;">
                <h5>Real-World Impact: Connection Pooling</h5>
                <p><strong>Typical production scenario:</strong> Metrics agent sends 10 metrics/second to server</p>
                <ul>
                    <li><strong>Without Keep-Alive:</strong> 10 connections/sec × 0.5ms = 5ms connection overhead/sec</li>
                    <li><strong>With Keep-Alive:</strong> 1 connection × 10 requests × 0.25ms = 2.5ms total/sec</li>
                    <li><strong>Savings:</strong> 50% latency reduction + 10x fewer connections</li>
                </ul>
                <p style="margin-top: 10px; color: #f39c12;"><strong>Lesson:</strong> HTTP Keep-Alive is essential for high-throughput services. HTTP/1.1 defaults to it for a reason!</p>
            </div>

            <div class="success-box" style="margin-top: 30px;">
                <h4>What You Learned</h4>
                <ul>
                    <li><strong>TCP Handshake Overhead:</strong> 3-way handshake adds significant latency</li>
                    <li><strong>HTTP Keep-Alive:</strong> Persistent connections amortize connection cost</li>
                    <li><strong>Listen Backlog Tuning:</strong> Kernel queue size critical for handling bursts</li>
                    <li><strong>Connection Lifecycle:</strong> Idle timeouts, graceful close, error handling</li>
                </ul>
            </div>

            <h4 style="margin-top: 20px;">Additional Resources</h4>
            <div class="info-box">
                <ul>
                    <li><a href="docs/phase7_keep_alive_results.md" target="_blank" style="color: #f39c12;">Phase 7 Detailed Results</a> - Complete performance analysis and test methodology</li>
                    <li><a href="learning-resources/concepts/connection_explained.txt" target="_blank" style="color: #f39c12;">TCP Connection Mechanics</a> - Deep dive into kernel listen queues and connection draining</li>
                </ul>
            </div>

            <div class="warning-box" style="margin-top: 30px;">
                <h4>🎯 Why Only Phase 1 Has Interactive Tutorial?</h4>
                <p>Phase 1 establishes the foundation: threading, hands-on coding, and measurement methodology. Once you understand these concepts, Phases 2-7 follow the same pattern:</p>
                <ol style="margin-top: 10px;">
                    <li><strong>Measure:</strong> Identify bottleneck through profiling</li>
                    <li><strong>Optimize:</strong> Implement targeted solution</li>
                    <li><strong>Verify:</strong> Measure again to confirm improvement</li>
                </ol>
                <p style="margin-top: 15px;">The codebase contains full implementations of all phases. You can explore the code, read documentation, and see the progressive optimization journey from 88% → 100% success rate at 2,253 RPS, and then the quantum leap to 145,348 RPS with event-driven I/O in Phase 8.</p>
            </div>
        </div>

        <!-- Phase 8 Panel -->
        <div class="content-panel" id="phase8">
            <h2>⚡ Phase 8: Event-Driven Architecture with epoll</h2>

            <div class="success-box" style="margin-bottom: 30px;">
                <h3 style="margin-bottom: 15px;">🎉 Spectacular Results: 64x Performance Increase!</h3>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-top: 20px;">
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 10px; text-align: center;">
                        <div style="font-size: 2.5em; font-weight: bold; color: #27ae60;">145,348</div>
                        <div style="margin-top: 5px; opacity: 0.9;">RPS (vs 2,253)</div>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 10px; text-align: center;">
                        <div style="font-size: 2.5em; font-weight: bold; color: #27ae60;">100%</div>
                        <div style="margin-top: 5px; opacity: 0.9;">Success Rate</div>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 10px; text-align: center;">
                        <div style="font-size: 2.5em; font-weight: bold; color: #27ae60;">1000+</div>
                        <div style="margin-top: 5px; opacity: 0.9;">Concurrent Clients</div>
                    </div>
                    <div style="background: rgba(255,255,255,0.1); padding: 20px; border-radius: 10px; text-align: center;">
                        <div style="font-size: 2.5em; font-weight: bold; color: #27ae60;">4.8ms</div>
                        <div style="margin-top: 5px; opacity: 0.9;">Avg Latency @ 1000</div>
                    </div>
                </div>
            </div>

            <h3>The Fundamental Shift: Blocking → Event-Driven I/O</h3>
            <p style="margin: 15px 0; line-height: 1.8;">
                Phase 8 represents a <strong>fundamental architectural transformation</strong> from blocking I/O to event-driven programming using Linux's <code>epoll</code>.
                Instead of dedicating one thread per connection (which wastes 80% of CPU time waiting for I/O), we use a single event loop to monitor thousands of connections
                and only process when data is actually ready.
            </p>

            <div class="info-box" style="margin: 20px 0;">
                <h4>What Changed</h4>
                <table style="width: 100%; margin-top: 15px; border-collapse: collapse;">
                    <thead>
                        <tr style="background: rgba(0,0,0,0.1);">
                            <th style="padding: 12px; text-align: left; border-bottom: 2px solid rgba(0,0,0,0.2);">Aspect</th>
                            <th style="padding: 12px; text-align: left; border-bottom: 2px solid rgba(0,0,0,0.2);">Phase 7 (Blocking)</th>
                            <th style="padding: 12px; text-align: left; border-bottom: 2px solid rgba(0,0,0,0.2);">Phase 8 (Event-Driven)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);"><strong>I/O Model</strong></td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">Blocking accept(), read(), write()</td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">Non-blocking with epoll event notification</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);"><strong>Threads</strong></td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">1 accept thread + 16 workers</td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">1 event loop + 16 workers (CPU only)</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);"><strong>Memory/Connection</strong></td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">8 MB (thread stack)</td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">4 KB (connection state)</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);"><strong>Event Notification</strong></td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">O(n) thread wake-ups</td>
                            <td style="padding: 10px; border-bottom: 1px solid rgba(0,0,0,0.1);">O(1) epoll_wait()</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px;"><strong>CPU Efficiency</strong></td>
                            <td style="padding: 10px;">~20% (80% blocked waiting)</td>
                            <td style="padding: 10px;">~95% (only work when ready)</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h3>Implementation Deep Dive</h3>

            <h4>New Components</h4>
            <div class="code-block">
<span class="comment">// Event Loop with epoll (src/event_loop.cpp)</span>
class EventLoop {
    int epoll_fd_;
    std::unordered_map&lt;int, std::unique_ptr&lt;Connection&gt;&gt; connections_;
    std::unique_ptr&lt;ThreadPool&gt; thread_pool_;

    void event_loop(int listen_fd) {
        while (running_) {
            <span class="highlight">// Wait for events - DOES NOT BLOCK resources!</span>
            int nfds = epoll_wait(epoll_fd_, events, MAX_EVENTS, timeout);

            for (int i = 0; i &lt; nfds; i++) {
                if (events[i].data.fd == listen_fd) {
                    handle_accept(listen_fd);  <span class="comment">// Non-blocking!</span>
                } else {
                    handle_read(events[i].data.fd);  <span class="comment">// Data ready!</span>
                }
            }
        }
    }
};</div>

            <h4>Key Features Implemented</h4>
            <ul style="margin: 15px 0; line-height: 1.8;">
                <li><strong>Non-blocking sockets:</strong> Set <code>O_NONBLOCK</code> flag on all sockets</li>
                <li><strong>Edge-triggered epoll:</strong> <code>EPOLLET</code> flag for maximum efficiency</li>
                <li><strong>HTTP request parsing:</strong> Proper boundary detection (<code>\r\n\r\n</code>) and Content-Length handling</li>
                <li><strong>Connection state management:</strong> Track partial requests, keep-alive status</li>
                <li><strong>Hybrid architecture:</strong> Event loop for I/O, thread pool for CPU work</li>
            </ul>

            <h3>Performance Benchmarks</h3>
            <div class="code-block">
<span class="comment"># Test 1: Light load (20 clients, 50 req each)</span>
./build/load_test_persistent 8080 20 50 1000
<span class="highlight">Result: 9,900 RPS, 100% success, 0.16ms latency</span>

<span class="comment"># Test 2: Medium load (50 clients, 100 req each)</span>
./build/load_test_persistent 8080 50 100 1000
<span class="highlight">Result: 24,630 RPS, 100% success, 0.17ms latency</span>

<span class="comment"># Test 3: High load (100 clients, 100 req each)</span>
./build/load_test_persistent 8080 100 100 1000
<span class="highlight">Result: 45,454 RPS, 100% success, 0.42ms latency</span>

<span class="comment"># Test 4: Very high load (200 clients, 100 req each)</span>
./build/load_test_persistent 8080 200 100 1000
<span class="highlight">Result: 83,682 RPS, 100% success, 0.55ms latency</span>

<span class="comment"># Test 5: Extreme load (500 clients, 100 req each)</span>
./build/load_test_persistent 8080 500 100 1000
<span class="highlight">Result: 137,362 RPS, 100% success, 1.66ms latency</span>

<span class="comment"># Test 6: Maximum stress (1000 clients, 100 req each)</span>
./build/load_test_persistent 8080 1000 100 1000
<span class="highlight">Result: 145,348 RPS, 100% success, 4.81ms latency ✅</span>

<span class="comment"># Zero failures across all 186,000 test requests!</span></div>

            <div class="success-box" style="margin-top: 30px;">
                <h4>What You Learned</h4>
                <ul>
                    <li><strong>Event-Driven Programming:</strong> The reactor pattern and how epoll achieves O(1) scalability</li>
                    <li><strong>Non-blocking I/O:</strong> Why blocking wastes CPU and how to avoid it</li>
                    <li><strong>C10K Problem:</strong> How to handle 10,000+ concurrent connections on commodity hardware</li>
                    <li><strong>Hybrid Architecture:</strong> Combining event loops (for I/O) with thread pools (for CPU work)</li>
                    <li><strong>HTTP Parsing:</strong> Request boundary detection and incremental parsing</li>
                    <li><strong>Memory Efficiency:</strong> 2000x reduction in memory per connection (8MB → 4KB)</li>
                </ul>
            </div>

            <h4 style="margin-top: 20px;">Interactive Visualizations</h4>
            <div class="info-box">
                <ul>
                    <li><a href="docs/phase8_architecture_visual.html" target="_blank" style="color: #f39c12;">Phase 8 Architecture Visual</a> - Interactive 5-tab comparison of blocking vs event-driven I/O</li>
                    <li><a href="docs/blocking_io_explained.html" target="_blank" style="color: #f39c12;">Blocking I/O Deep Dive</a> - Why blocking wastes CPU cycles (with animations!)</li>
                    <li><a href="docs/metric_transformation_visual.html" target="_blank" style="color: #f39c12;">Metric Journey Visualization</a> - See how a metric transforms through all 7 stages from user app → storage</li>
                    <li><a href="../phase8_results.md" target="_blank" style="color: #f39c12;">Phase 8 Detailed Results</a> - Complete performance analysis and architecture documentation</li>
                    <li><a href="../docs/end_to_end_flow.md" target="_blank" style="color: #f39c12;">End-to-End Flow Guide</a> - Complete technical documentation of metric ingestion pipeline</li>
                </ul>
            </div>

            <div class="warning-box" style="margin-top: 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border: none; color: white;">
                <h4 style="color: white;">🚀 Why This Matters</h4>
                <p style="margin: 15px 0; line-height: 1.8;">
                    This is the <strong>exact architecture</strong> used by production systems at massive scale:
                </p>
                <ul style="line-height: 1.8;">
                    <li><strong>nginx:</strong> Web server serving millions of requests/sec</li>
                    <li><strong>Redis:</strong> In-memory database with 100K+ ops/sec</li>
                    <li><strong>Node.js:</strong> JavaScript runtime for I/O-heavy applications</li>
                    <li><strong>HAProxy:</strong> Load balancer handling massive concurrent traffic</li>
                </ul>
                <p style="margin: 15px 0; line-height: 1.8;">
                    You've now built the fundamental pattern that enables modern high-performance network services to scale to thousands of concurrent connections on a single machine.
                    This is <strong>production-grade systems engineering</strong>.
                </p>
            </div>
        </div>

        <div class="content-panel" id="future">
            <h2>🔮 Future Milestones: Complete the Platform</h2>

            <div class="info-box" style="background: #1a1a2e; border-left: 4px solid #3498db; margin-bottom: 30px;">
                <h4 style="color: #3498db; margin-bottom: 15px;">Milestone 2: Message Queue Integration</h4>
                <p style="color: #ecf0f1; margin-bottom: 10px;"><strong>Goal:</strong> Replace file I/O with Kafka for distributed, scalable ingestion</p>
                <ul style="color: #bdc3c7;">
                    <li>Apache Kafka setup and partitioning strategy</li>
                    <li>Producer optimizations (batching, compression, acks)</li>
                    <li>Topic design and retention policies</li>
                    <li>Monitoring and metrics for broker health</li>
                </ul>
                <p style="margin-top: 15px; color: #7f8c8d;"><strong>Expected Performance:</strong> 50K+ RPS per ingestion server, with horizontal scalability across Kafka partitions</p>
            </div>

            <div class="info-box" style="background: #1a1a2e; border-left: 4px solid #e67e22; margin-bottom: 30px;">
                <h4 style="color: #e67e22; margin-bottom: 15px;">Milestone 3: Stream Processing</h4>
                <p style="color: #ecf0f1; margin-bottom: 10px;"><strong>Goal:</strong> Real-time aggregations and alerting with Apache Flink</p>
                <ul style="color: #bdc3c7;">
                    <li>Flink streaming jobs for windowed aggregations</li>
                    <li>Stateful stream processing with checkpointing</li>
                    <li>Alert rules engine with threshold detection</li>
                    <li>Event time vs processing time handling</li>
                </ul>
                <p style="margin-top: 15px; color: #7f8c8d;"><strong>Expected Capabilities:</strong> 1-minute aggregation windows, sub-second alert latency, exactly-once processing guarantees</p>
            </div>

            <div class="info-box" style="background: #1a1a2e; border-left: 4px solid #9b59b6;">
                <h4 style="color: #9b59b6; margin-bottom: 15px;">Milestone 4: Storage & Visualization</h4>
                <p style="color: #ecf0f1; margin-bottom: 10px;"><strong>Goal:</strong> Time-series storage and dashboards with InfluxDB + Grafana</p>
                <ul style="color: #bdc3c7;">
                    <li>InfluxDB schema design and retention policies</li>
                    <li>Continuous queries for downsampling</li>
                    <li>Grafana dashboards with variable templating</li>
                    <li>Query optimization for time-series data</li>
                </ul>
                <p style="margin-top: 15px; color: #7f8c8d;"><strong>Expected Capabilities:</strong> 30-day full retention, 1-year downsampled retention, sub-second query latency for recent data</p>
            </div>

            <div class="warning-box" style="margin-top: 30px;">
                <h4>📚 Why Milestones Are Planned but Not Built Yet</h4>
                <p>This project focuses on <strong>deep learning through implementation</strong> rather than breadth. Milestone 1 (Ingestion Layer) contains 7 optimization phases—each teaching concurrency, I/O, profiling, and performance analysis.</p>
                <p style="margin-top: 10px;">Building all 4 milestones would create a complete monitoring platform, but the educational value is in <em>how you build it</em>, not just shipping features. Future milestones will be added as the foundation solidifies.</p>
            </div>
        </div>

        <div class="content-panel" id="decision-tree">
            <h2>🌳 Interactive Decision Tree</h2>
            <p style="font-size: 1.1em; color: #7f8c8d; margin-bottom: 30px;">
                Explore all 7 optimization phases and their architectural decisions.
            </p>
            <div class="info-box">
                <p><strong>Decision tree visualization will be embedded here.</strong></p>
                <p>For now, view it separately at: <a href="docs/index.html" target="_blank">Decision Tree Visualization</a></p>
            </div>
        </div>

        <div class="footer">
            <p style="font-size: 1.2em; margin-bottom: 10px;">
                <strong>Systems Craft</strong>
            </p>
            <p style="opacity: 0.8;">
                Build production systems. Learn by doing. Measure everything.
            </p>
            <p style="margin-top: 20px; opacity: 0.7; font-size: 0.9em;">
                View source code: <a href="https://github.com/kapil0x/MetricsStream" style="color: #3498db;">GitHub Repository</a>
            </p>
        </div>
    </div>

    <script>
        function showPhase(phaseId) {
            // Hide all content panels
            document.querySelectorAll('.content-panel').forEach(panel => {
                panel.classList.remove('active');
            });

            // Remove active class from all tabs
            document.querySelectorAll('.phase-tab').forEach(tab => {
                tab.classList.remove('active');
            });

            // Show selected panel
            document.getElementById(phaseId).classList.add('active');

            // Highlight active tab
            event.target.classList.add('active');

            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        function toggleSolution(solutionId) {
            const solution = document.getElementById(solutionId);
            const button = event.target;

            if (solution.classList.contains('solution-hidden')) {
                // Show solution
                solution.classList.remove('solution-hidden');
                solution.classList.add('solution-visible');
                button.textContent = button.textContent.replace('Show', 'Hide');
                button.style.background = 'linear-gradient(135deg, #95a5a6 0%, #7f8c8d 100%)';
            } else {
                // Hide solution
                solution.classList.remove('solution-visible');
                solution.classList.add('solution-hidden');
                button.textContent = button.textContent.replace('Hide', 'Show');
                button.style.background = 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)';
            }
        }

        function toggleArchitecture(phaseId) {
            const beforeEl = document.getElementById(phaseId + '-before');
            const afterEl = document.getElementById(phaseId + '-after');

            if (beforeEl.style.display === 'none') {
                // Show before, hide after
                beforeEl.style.display = 'block';
                afterEl.style.display = 'none';
            } else {
                // Show after, hide before
                beforeEl.style.display = 'none';
                afterEl.style.display = 'block';
            }
        }
    </script>
</body>
</html>
